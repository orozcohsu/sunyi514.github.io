<!doctype html>
<html class="theme-next use-motion theme-next-mist">
<head>
  

<meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>


<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />








  <link rel="stylesheet" type="text/css" href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5"/>




<link rel="stylesheet" type="text/css" href="/css/main.css?v=0.4.5.1"/>




  <meta name="keywords" content="Hexo,next" />





  <link rel="shorticon icon" type="image/x-icon" href="/favicon.ico?v=0.4.5.1" />


<meta name="description">
<meta property="og:type" content="website">
<meta property="og:title" content="奔跑的兔子">
<meta property="og:url" content="http://sunyi514.github.io/index.html">
<meta property="og:site_name" content="奔跑的兔子">
<meta property="og:description">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="奔跑的兔子">
<meta name="twitter:description">


<script type="text/javascript" id="hexo.configuration">
  var CONFIG = {
    scheme: 'Mist',
    sidebar: 'post'
  };
</script>

  <title> 奔跑的兔子 </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  <!--[if lte IE 8]>
  <div style=' clear: both; height: 59px; padding:0 0 0 15px; position: relative;margin:0 auto;'>
    <a href="http://windows.microsoft.com/en-US/internet-explorer/products/ie/home?ocid=ie6_countdown_bannercode">
      <img src="http://7u2nvr.com1.z0.glb.clouddn.com/picouterie.jpg" border="0" height="42" width="820"
           alt="You are using an outdated browser. For a faster, safer browsing experience, upgrade for free today or use other browser ,like chrome firefox safari."
           style='margin-left:auto;margin-right:auto;display: block;'/>
    </a>
  </div>
<![endif]-->
  
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-60954885-1', 'auto');
  ga('send', 'pageview');
</script>




  <div class="container one-column 
   page-home 
">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><h1 class="site-meta">
  <span class="logo-line-before"><i></i></span>
  <a href="/" class="brand" rel="start">
      <span class="logo">
        <i class="icon-next-logo"></i>
      </span>
      <span class="site-title">奔跑的兔子</span>
  </a>
  <span class="logo-line-after"><i></i></span>
</h1>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu ">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            <i class="menu-item-icon icon-next-home"></i> <br />
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            <i class="menu-item-icon icon-next-categories"></i> <br />
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            <i class="menu-item-icon icon-next-about"></i> <br />
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            <i class="menu-item-icon icon-next-archives"></i> <br />
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            <i class="menu-item-icon icon-next-tags"></i> <br />
            标签
          </a>
        </li>
      

      
      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div id="content" class="content"> 
  <section id="posts" class="posts-expand">
    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2015/08/21/elements-of-scale阅读笔记/" itemprop="url">
                Elements of Scale阅读笔记
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2015-08-21T20:22:12+08:00" content="2015-08-21">
            2015-08-21
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/data-system/" itemprop="url" rel="index">
                  <span itemprop="name">data system</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2015/08/21/elements-of-scale阅读笔记/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2015/08/21/elements-of-scale阅读笔记/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><p>原文地址：<a href="http://www.benstopford.com/2015/04/28/elements-of-scale-composing-and-scaling-data-platforms/" target="_blank" rel="external">http://www.benstopford.com/2015/04/28/elements-of-scale-composing-and-scaling-data-platforms/</a></p>
<p>前两天看了下这篇文章（后面简称原文），感觉不错，原本打算完整的翻译一下，结果发现已经有人这么做了（<a href="http://blog.jobbole.com/88453/" target="_blank" rel="external">传送门</a>，不过有一些地方翻译存在问题）。所以还是写一篇总结性质的笔记吧。</p>
<h1 id="正文">正文</h1><p>过去十年，技术界为了解决与数据平台有关的问题，诞生了很多的方案。如今的数据平台架构已经非常复杂。本文的内容就是剖析一下其中一些相对成熟有效，得到较广泛使用的方案背后的原理。</p>
<h1 id="局部性">局部性</h1><p>所谓处理数据，其实就是如何去利用局部性。利用局部性可以让CPU做出预测，更好的缓存数据，因为可以做预读取。对于硬盘来说，顺序读取由于可以利用局部性，让数据缓存到disk buffer，page cache，L1~L3 cache等各个层级的缓存，而随机读则无法利用缓存（其实也会预读取，但是预读取的数据后面就用不到了，等同于失效），所以顺序读优于随机读。而且不仅如此，其实对于内存来说顺序读的速度也比随机读快一两个数量级，如果考虑GC的代价，差距更大。另外，磁盘顺序读比内存随机读速度还要快点（具体数据参考<a href="http://deliveryimages.acm.org/10.1145/1570000/1563874/jacobs3.jpg" target="_blank" rel="external">此图</a>）。虽然这几年SSD逐渐普及开来，表现出不太一样的特性（没有磁盘的寻道时间，所以随机IO速度快），但是在缓存这件事情上还是一样的。</p>
<p><img src="http://benstopford.com/uploads/img/Slide04.png" alt="locality"></p>
<h1 id="存储引擎">存储引擎</h1><p>来看一下如何构造基本的单机存储引擎。记住我们之前讨论的结果：顺序IO优于随机IO。我们从最基本的文件实现开始，数据保存在一个文件里面，新增记录append到数据尾部。这种存储方式在数据库中叫heap file。数据的更新采用日志记录方式（所以也是顺序写入），然后在读取的时候做merge。</p>
<p><img src="http://benstopford.com/uploads/img/Slide11.png" alt="update"></p>
<p>然后是增加索引，如BTree，这样查询数据不用scan整个数据文件。增加索引之后发现，数据的写入需要同时更新索引，而由于索引是有序的，所以这种更新是随机写。这样一来速度开始变慢了，在磁盘上维护索引的写入性能比没有用索引可能要差1000倍。那么我们就要开始考虑解决方案。</p>
<p>最容易想到的方案A，就是把索引放在内存维护。像MongoDB，CouchBase，Riak等系统都用的这种方案，具体来说会用到mmap（内存映射文件）。这种方案简单快捷，但是有一个问题在于：内存大小相对有限，如果数据量太大，在内存里就装不下索引了。</p>
<p>所以接下来的方案B，为了维护更大的索引，可以把索引拆成多个。具体来说，每次先在内存维护写入的数据，等到积攒到一定大小了，就整理成一个索引文件写入磁盘。这样就会有一堆索引文件，当然后台也需要定期合并，避免索引过多和内容重复。在实际系统中，HBase和Cassandra都用了这个方案，具体来说用到的技术是LSM Tree。 这个方案的问题在于，读取时需要读取多个索引，而且索引里面可能有重复更新的过期内容，这都会增加读取的数据量，使得读性能下降。这相当于把问题从随机写转移到随机读。为了解决这个问题，可以增加一些元索引，或者bloom filter之类的。</p>
<p>还有一种方案C，专门面向分析类应用的，可以把上面说的索引都去掉，直接按列存储。这样一来读取数据的时候，可以不用读取不需要的列。另外列存储会有更加高效的编码压缩（比如最典型的有LRE，delta等），而且查询时的filter可以直接作用于编码压缩之后的数据。再者列存储一般都按一定大小的block计算一些统计值，filter和aggregation也可以直接利用这些统计值，直接跳过对block的读取。典型的列存储格式就是从hadoop生态圈发展起来的parquet和orc，一些MPP数据库如vertica也有自己的列存储格式。列存储的写入，由于要写入多个文件，所以理论上会比行存储效率低，不过也可以用类似LSM的策略（但不是类似的结构）解决写入的问题。最后补充一个实践经验：分析类应用如果有数据update需求，很有可能是大批量的数据更新，这种情况下虽然数据库即使先在内存保存了更新数据，使得更新操作速度尚可，但是读取数据的时候就要做大量合并（原来的更新会被当成为delete + insert，delete的数据会在读取时合并），这样还是会降低读取速度，所以建议尽量少做大批量update/delete，或者做完之后调用数据刷新的接口（如果系统提供的话）重新整理数据。</p>
<p>如果要说列存储有没有跟行存储类似性质的索引，原文中认为列存储中每一列的每行字段都会有rowid，算是类似的概念，这也是列存储和行存储用多个索引读取数据的区别：列存储只需要多个列相同rowid的字段进行join，属于顺序读。行存储使用两个索引的情况，比如数据存储使用index file（主索引即数据）而不是前面讨论的heap file（比如mysql中的innodb v.s. myisam），那么使用二级索引其实是要根据二级索引去查主索引，主索引再查到数据，还是需要随机读。相比之下自然是列存储的方式更加高效。</p>
<p><img src="http://benstopford.com/uploads/merge.png" alt="merge_join"></p>
<p>除了上面的解决方案以外，还要提一下消息队列。现在在大数据系统中流行的kafka跟传统的消息中间件不太一样，传统中间件的规范如JMS，AMQP都要求消息队列有索引。如果加上索引，那么从其实已经跟数据库很像了。事实上数据库大师Jim Gray早在95年就发文指出<a href="http://research.microsoft.com/pubs/69641/tr-95-56.pdf" target="_blank" rel="external">消息队列即数据库</a>。</p>
<p>以上展示了存储引擎实现的基本原理，当然真实系统中的情况还要更加的复杂，有很多细节的需要深入。下图罗列了刚才讨论的所有方案：</p>
<p><img src="http://benstopford.com/uploads/four.jpg" alt="storage"></p>
<h1 id="并行">并行</h1><p>数据平台面临的并行问题就是如何把数据分布到多台机器上。方法就两种，一种是分片（partition，也可以叫sharding），另一种是复制（replication）。</p>
<p>分片常见的方式如hash分片（比如按照key），或者范围分片（比如按照日期）。分片可以解决两类问题：</p>
<blockquote>
<ul>
<li>一类是按照分片的key获取数据，这样可以只访问部分机器，每个机器只需要处理一部分数据的请求，于是就增加了并发能力。</li>
<li>另一类是批处理计算，比如报表计算，机器学习模型训练等等，数据分布在不同机器上，可以启动所有机器的计算资源，相当于对计算任务分而治之。当然这种场景下单个计算任务（理论上）就能耗费所有系统资源，所以任务并发能力较低。</li>
</ul>
</blockquote>
<p><img src="http://7xltg5.com1.z0.glb.clouddn.com/partition.jpg" alt="partition"></p>
<p>然而，对于另外一些场景，比如按照二级索引取数据，分片就不那么好使了，因为二级索引需要扫描所有的机器，于是这就会限制并发能力。所以有些系统如HBase就不提供这种功能（虽然不提供统一的内置功能，但实际上还是有一些<a href="http://hbase.apache.org/book.html#secondary.indexes" target="_blank" rel="external">变通方法</a>），但是另外一些系统如MongoDB还是会提供，毕竟这是一个很强烈的需求。要解决这个问题，就要复制出场了。</p>
<p><img src="http://7xltg5.com1.z0.glb.clouddn.com/replication.jpg" alt="replication"></p>
<p>复制按照类型可以分为不可见复制（只用来做备份恢复），只读复制（增加读并发），读写复制（增加读写并发，也增加了网络分区时的可用性）。只读复制适用于MS架构，读写复制适用于对等的P2P架构。选用哪一种复制模式会影响到一致性的程度。当然这里说的一致性是CAP中的一致性：所有节点同一时间看到相同数据，简单来说就是强一致性。</p>
<p>考虑到CAP理论，如果要维持强一致性，那么整个系统就变成龟速（牺牲可用性）。在具有事务（ACID）的传统数据库中，也是有保证一致性的方式的，叫做serializability。但实际上由于代价太大，在事务隔离级别中，很多数据库根本不支持这个级别，即使支持，也<a href="http://www.benstopford.com/default-and-maximum-isolation-levels-for-databases-january-2013/" target="_blank" rel="external">很少</a>是将其设为默认值的。（注意：原文中作者用了linearizability而非serializability，实际上两者是<a href="http://www.bailis.org/blog/linearizability-versus-serializability/" target="_blank" rel="external">有所不同的</a>。linearizability完全匹配CAP中的C含义，故而作者会用这个词。serializability是数据库界提出的，围绕事务概念，要求事务总顺序串行。linearizability则是搞分布式系统和并发编程的人提出的，没有事务概念，要求对单个对象的单个操作按照严格顺序。<a href="https://en.wikipedia.org/wiki/Linearizability#Linearizability_versus_serializability" target="_blank" rel="external">这里</a>有一个例子，可以参考对比。）</p>
<p>虽然强一致性很难做到，但是这并不代表就要放弃了。以下是一些解决方案：</p>
<blockquote>
<ul>
<li>把数据变成不可变（immutable）的。最简单的就比如流量日志数据。复杂一点的情况，就是看起来需要更新状态，实际上可以转化成不可变数据。比如把一个交易标记为可能是欺诈交易，看起来好像需要更新字段，但实际上可以通过用新的单独数据来标记，关联到原来的交易。</li>
<li>隔离写操作到单台机器，如<a href="http://www.datomic.com/" target="_blank" rel="external">Datomic</a>。</li>
<li>学术界提出的理论方案：基于<a href="http://www.bloom-lang.net/calm/" target="_blank" rel="external">CALM</a>原理设计的Bloom语言。用这个语言编程的程序可以用代码分析技术检测出是否会造成不一致，对不一致的代码块可以由程序员人为调整加入coordination，从而获得一致性。当然，这个方案目前看来还是比较“高冷”，不能落地。</li>
</ul>
</blockquote>
<p>不过很明显这些方法都只能适用于部分需求，实际需求中很多情况下还是不得不放弃强一致性，采取最终一致性。</p>
<h1 id="架构">架构</h1><p>通过前面的讨论，我们已经把系统从单机扩展到分布式。我们提出了很多问题的解决方案，讨论了各种tradeoff，现在是时候把这些方案都组装起来成为一个完整的数据平台架构了。</p>
<p>最简单的架构就是一个单独的数据库系统，读写都从里面出。这种模式只能应对小规模的负载。系统吞吐要求上去之后，就要考虑异步（如message passing，actor）和负载均衡。另外这种模式把系统（即数据库）当成黑盒使用。数据库是有很多不错的特性，但是事务的存在让数据库没有太好的办法处理扩展性。在事务里面操作很安全，但是强制让事务存在以至于扩展性受限就不好了，尤其是很多需求可以不用事务来实现。</p>
<p>我们从最简单的方案开始考虑：CQRS。CQRS也就是读写分离，写操作和读操作分别对应不同的存储单元，这样处理写操作的单元可以专门使用可以优化写操作的存储结构，读操作这边也可以优化。比较简单的情形如MongDB使用复制集完成读写分离，复杂一点的如时间序列数据库druid，把节点分成realtime和history，realtime实时接入数据流，针对写操作优化。history则属于专门用来查询的，也可以批量加载从realtime过来的和单独加载的数据，针对读优化。查询时为了获取最新数据，realtime和history都需要查询（当然这点对于用户是屏蔽的，druid中用户查询是有专门的broker节点作为中间路由）。realtime虽然没有特别的读优化，但是数据量不大（定期转移到history，实践中一般按小时级别），所以查询不会有太大问题。</p>
<p><img src="http://7xltg5.com1.z0.glb.clouddn.com/cqrs.jpg" alt="cqrs"></p>
<p>接下来作者又提出了Operational/Analytic Bridge架构（后面简称OAB），这是作者在银行工作期间搭建平台时逐渐抽象出来的（题外话：作者现在去了Confluent，也就是Kafka作者Jay Kreps的公司）。其实之前Jay Kreps在The Log一文中已经提出过类似架构，只不过本文中对细节描述的更加清楚。如下图所示，左边这一部分是Operational的，面向OLTP环境，也就是支持读写，也可以支持事务，右边这部分属于Analytic，面向OLAP环境，针对只读数据做优化。中间的数据流是由Operational的写操作同步产生，Analytic则以消息流作为桥接，异步、批量的进行读操作。虽然作者在原文里没有提到OAB架构跟CQRS之间有没有演化关系，但是我想如果有的话，那么可以这样理解：把CRQS中写优化节点和读优化节点的数据同步机制强化，做成比较健壮的消息层，原来的读写两类节点能接入更多不同系统，这就算进阶到OAB架构了。</p>
<p><img src="http://benstopford.com/uploads/img/Slide39.png" alt="OAB"></p>
<p>基于OAB架构的原型，可以开始强化只读层的系统。为了支持更大的数据处理规模，就要引入hadoop，由hadoop做计算，数据库做查询。但是单单只有hadoop做计算，那么只能算一个批处理管道（batch pipeline），数据的时效性有滞后。如果想更快的查到结果，就要引入流式计算。<br>结合批处理和流式计算的架构，叫做Lambda架构，数据分别由批处理层(Hadoop)和实时层(Storm)分别计算，批处理层有准确的历史数据，可以反复计算。实时层进行增量数据的计算，但是由于只能容纳一定的计算窗口，所以数据不完全精确（比如uv数据，增量数据就不能代替全量数据），但是通过批处理层最终还是能得到精确数据。应用端查询全量数据时同时查询批处理层的结果数据和实时层的结果数据。</p>
<p>如果把Lambda架构的离线计算层去掉，就变成了Kappa架构，作者在原文中叫流数据平台（stream data platform）。出现Kappa架构的原因是因为Jay Kreps认为Lambda架构有一个最大的问题，就实时和离线其实是两套相同的逻辑，首先计算部分的逻辑是相同的，但是用了不同的计算平台，其次两边的结果表也是相同的，但是一般也放在不同的系统中。这两个环节都拥有相同的两套逻辑，会对数据处理和查询逻辑的维护带来很大负担。一方面是代码维护的负担，这或许可以通过一些编程框架解决，另一方面是两套系统和数据调试的负担，这个就不好解决了。所以Kappa架构中直接就去掉了离线计算层（不过hadoop还是可以保留的，只不过不再起到Lambda架构中的作用），数据直接存在消息队列kafka中，所有计算都通过流计算完成。如果需要重新计算历史数据，只需要另外开一个流计算任务从比较早的kafka数据开始追，追上了之后原来的老结果表就可以被新结果表取代。这种架构在速度上不见得会比Lambda架构更快，但是能解决前面提出的问题。其最大的缺点在于kafka中必须保持很大的数据量，只有确定数据不会再需要重新计算，才会过期。而且流式计算的窗口可能会很大。</p>
<p><img src="http://7xltg5.com1.z0.glb.clouddn.com/batch_stream.jpg" alt="batch_stream"></p>
<p>最后，用Kappa架构结合OAB的整体架构再从全局回顾，困扰技术供应商多年的“应用集成”问题，或许会有潜在的解决可能性。大型技术供应商如Informatica，Tibco，Orcale都为应用集成提出一些方案，但是通常只能解决局部问题，或者表面上看起来解决了，其实没有彻底的解决。这些传统的解决方案，如基于数据的同步，数据集成(ETL/数据仓库)，甚至最新的所谓数据湖概念，都由于在复制层吞吐量不够大，以及很难处理schema变更而碰到阻碍。但是我们上面谈到的架构，有了kafka这一层高吞吐的消息队列作为单独的中间环节，把一致性问题隔离在数据源这层，并且kafka还可以作为历史数据记录，能够通过重放日志恢复状态，不需要像数据库那样做检查点（checkpoint）。所以这个架构至少能解决前一个问题，而后一个问题（schema变更）倒是还真的没有定论。</p>
<p><img src="http://benstopford.com/uploads/sdp.jpg" alt="EAI"></p>
<h1 id="总结">总结</h1><p>我们从单个组件开始讨论，其关键点在于利用顺序读写产生局部性。然后我们对单个组件做水平扩展，利用分片和复制（小图6）。这个时候出现了一致性问题，在构建平台的时候需要将其隔离（小图4）。</p>
<p>接下来把视野扩大到整个数据平台，需要找到各个组件协同工作的平衡点，使得数据平台变成一个统一的整体（小图8）。为此，我们用增量重建（小图3），将系统从写优化转移到读优化（小图5），把一致性问题的约束通过异步只读的数据流来消解（小图1）。</p>
<p>系统中还会有一些别的问题要小心，原文中没有专门讲，一个是数据库schema对数据写入的约束（小图2，意思是写入时的检查），另一个是由分布式系统，尤其是异步环境下带来的时钟问题（小图7，会影响到并发顺序）。不过认真对待，还是能搞定的。</p>
<p>未来一定还会出现更多的解决方案，逐步融合到现有的数据平台中，解决更多已有或未知的问题。</p>
<p><img src="http://benstopford.com/uploads/conc1.jpg" alt="summary1"><br><img src="http://benstopford.com/uploads/conc2.jpg" alt="summary2"></p>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2015/03/28/mac-osx上安装hadoop2（续）/" itemprop="url">
                Mac OSX上安装Hadoop2（续）
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2015-03-28T17:28:11+08:00" content="2015-03-28">
            2015-03-28
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/data-system/" itemprop="url" rel="index">
                  <span itemprop="name">data system</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2015/03/28/mac-osx上安装hadoop2（续）/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2015/03/28/mac-osx上安装hadoop2（续）/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><p>一年之前曾经写过一篇<a href="http://sunyi514.github.io/2014/03/06/mac-osx%E4%B8%8A%E5%AE%89%E8%A3%85hadoop2/">Mac OSX上安装Hadoop2</a>，这两天就狗尾续貂一把，写一写上次遗漏的和一些新的东西。</p>
<p>在之前的文章中，使用的是2.3版本，但是目前hadoop已经发展到2.6版本。所以这段时间也已经把之前安装的hadoop2.3替换成了hadoop2.5，采用cdh5.3。</p>
<p>另外，要强调的是，这个安装笔记只是为了最快速度的让系统运行起来，所以会尽量的采用默认配置。</p>
<h1 id="MR_History_Server">MR History Server</h1><p>在之前的文章中，没有把MR History Server跑起来，这样的话至少会有两个问题：任务运行完成之后没法看在界面看到Task的日志，因为App Master运行结束之后就关闭了。另外hadoop重启或者超过指定时间之后，之前运行的job在Yarn主界面上都已经看不到了。而由于hadoop2中history server从resource manager中单独分离了出来，所以要单独启动。</p>
<p>开启MR的History Server，首先需要在<code>yarn-site.xml</code>里面配置log aggregation，并且指定log目录打开地址（如果不配置参数的话其实也能启动，但是在主界面上是打不开log内容的）：</p>
<figure class="highlight pf"><table><tr><td class="code"><pre><span class="line"><span class="variable">&lt;property&gt;</span></span><br><span class="line">    <span class="variable">&lt;name&gt;</span>yarn.<span class="keyword">log</span>.server.url<span class="variable">&lt;/name&gt;</span></span><br><span class="line">    <span class="variable">&lt;value&gt;</span>http://localhost:<span class="number">19888</span>/jobhistory/logs<span class="variable">&lt;/value&gt;</span></span><br><span class="line"><span class="variable">&lt;/property&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="variable">&lt;property&gt;</span></span><br><span class="line">    <span class="variable">&lt;name&gt;</span>yarn.log-aggregation-enable<span class="variable">&lt;/name&gt;</span></span><br><span class="line">    <span class="variable">&lt;value&gt;</span>true<span class="variable">&lt;/value&gt;</span></span><br><span class="line"><span class="variable">&lt;/property&gt;</span></span><br></pre></td></tr></table></figure>
<p>然后，运行如下命令即可启动：</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">./sbin/mr-jobhistory-daemon<span class="class">.sh</span> start historyserver</span><br></pre></td></tr></table></figure>
<p>现在，已完成的任务的Tracking URL将从App Master“移交给”History Server。另外在jobhistory的界面<code>localhost:19888</code>上也可以看到所有已经完成的任务：</p>
<p> <img src="http://7xltg5.com1.z0.glb.clouddn.com/mr-his.jpg" alt="mr-his"></p>
<p>另外，如果想从命令行来看日志，可以用yarn logs命令，比如典型的查看am的log，就是：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">yarn logs -applicationId <span class="tag">&lt;<span class="title">APP_ID</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h1 id="Spark_on_Yarn">Spark on Yarn</h1><p>如果说1年前要部署spark，可能还是倾向于独立部署。但是到了今天，相信spark on yarn在概念上已经不再陌生。</p>
<p>Spark on Yarn的基本安装很简单。首先确保scala环境已经安装，这里就不再赘述了。下载spark后（这里用的是<code>spark-1.3-hadoop-2.4</code>），在<code>conf/spark-env.sh</code>里面设置hadoop conf目录即可：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">export</span> HADOOP_CONF_DIR=/Users/sunyi/develop/hadoop-<span class="number">2.5</span><span class="number">.0</span>-cdh5<span class="number">.3</span><span class="number">.0</span>/etc/hadoop</span><br></pre></td></tr></table></figure>
<p>然后就可以开始运行spark job，不需要像独立部署时启动master/worker之类的了：</p>
<figure class="highlight crystal"><table><tr><td class="code"><pre><span class="line">./bin/spark-submit --<span class="class"><span class="keyword">class</span> <span class="title">org</span>.<span class="title">apache</span>.<span class="title">spark</span>.<span class="title">examples</span>.<span class="title">SparkPi</span> \</span></span><br><span class="line">    --master yarn-cluster \</span><br><span class="line">    --num-executors <span class="number">4</span> \</span><br><span class="line">    --driver-memory <span class="number">1</span>g \</span><br><span class="line">    --executor-memory <span class="number">1</span>g \</span><br><span class="line">    --executor-cores <span class="number">1</span> \</span><br><span class="line">    <span class="class"><span class="keyword">lib</span>/<span class="title">spark</span>-<span class="title">examples</span>-*.<span class="title">jar</span> \</span></span><br><span class="line">    <span class="number">100</span></span><br></pre></td></tr></table></figure>
<p>可以看到只需要指定master为yarn-cluster，并且给出一些运行参数即可。Spark的运行依赖spark-assembly和应用程序spark-examples会从本地上传。当然，可以用SPARK_JAR变量指定spark-assembly的位置，并且上传spark-assembly到hdfs：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">export</span> SPARK_JAR=hdfs:<span class="comment">//localhost:9000/spark-assembly-1.3.0-hadoop2.4.0.jar</span></span><br></pre></td></tr></table></figure>
<h1 id="Spark_History_Server">Spark History Server</h1><p>Spark同样有自己的History Server。配置方法是在conf目录下编辑<code>spark-default.conf</code>，加入如下3行：</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">spark<span class="class">.eventLog</span><span class="class">.enabled</span> 		 true</span><br><span class="line">spark<span class="class">.yarn</span><span class="class">.historyServer</span><span class="class">.address</span> localhost:<span class="number">18080</span></span><br><span class="line">spark<span class="class">.eventLog</span><span class="class">.dir</span>               hdfs:<span class="comment">//localhost:9000/spark-log</span></span><br></pre></td></tr></table></figure>
<p>这三行分别指定了开启日志记录，ui地址，以及日志目录（要提前建好目录）。然后就可以启动了：</p>
<figure class="highlight vim"><table><tr><td class="code"><pre><span class="line">./sbin/start-<span class="keyword">history</span>-server.<span class="keyword">sh</span>  hdf<span class="variable">s:</span>//localhos<span class="variable">t:9000</span>/spark-<span class="built_in">log</span></span><br></pre></td></tr></table></figure>
<p>这样，刚才我们的spark pi任务在运行结束后就可以在<code>localhost:18080</code>界面上看到了：</p>
<p> <img src="http://7xltg5.com1.z0.glb.clouddn.com/spark-his.jpg" alt="spark-his"></p>
<h1 id="Hadoop_Timeline_Server">Hadoop Timeline Server</h1><p>Timeline Server是hadoop2.4新引入的一个功能。由于hadoop2开始支持各种计算框架，而目前各个框架自己都提供独立的history server机制，所以Timeline Server的主要意义在于将各个框架自己提供的history server统一集成到一个history server中，方便用户统一查询已完成的所有任务，主要功能分为两块：</p>
<ul>
<li>提供已完成任务的通用信息，包括任务名，队列等等，这个跟yarn主界面看到的类似。</li>
<li>集中收集各个框架提供的任务信息，比如mapreduce的map数，reduce数，counter等等，以便进行统一查询。</li>
</ul>
<p>其中第二个功能目前还不太完善，只提供API方式获取数据，这里就配置一下第一个功能。配置方法就是在<code>yarn-site.xml</code>中添加如下配置开启：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">name</span>&gt;</span>yarn.timeline-service.generic-application-history.enabled<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">value</span>&gt;</span>true<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">name</span>&gt;</span>yarn.timeline-service.generic-application-history.store-class<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">value</span>&gt;</span>org.apache.hadoop.yarn.server.applicationhistoryservice.</span><br><span class="line">         FileSystemApplicationHistoryStore<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>然后启动Timeline Server：</p>
<figure class="highlight nginx"><table><tr><td class="code"><pre><span class="line"><span class="title">yarn</span> historyserver</span><br></pre></td></tr></table></figure>
<p>这样就可以打开默认界面<code>localhost:8188</code>，所有之前运行的mr和spark任务都已经可以在界面上看到：</p>
<p> <img src="http://7xltg5.com1.z0.glb.clouddn.com/hadoop-timeline.jpg" alt="hadoop-timeline"></p>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2014/11/15/盘点sql-on-hadoop中用到的主要技术/" itemprop="url">
                盘点SQL on Hadoop中用到的主要技术
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2014-11-15T14:41:30+08:00" content="2014-11-15">
            2014-11-15
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/data-system/" itemprop="url" rel="index">
                  <span itemprop="name">data system</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2014/11/15/盘点sql-on-hadoop中用到的主要技术/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2014/11/15/盘点sql-on-hadoop中用到的主要技术/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><p>自打Hive出现之后，经过几年的发展，SQL on Hadoop相关的系统已经百花齐放，速度越来越快，功能也越来越齐全。本文并不是要去比较所谓“交互式查询哪家强”，而是试图梳理出一个统一的视角，来看看各家系统有哪些技术上相通之处。</p>
<p>考虑到系统使用的广泛程度与成熟度，在具体举例时一般会拿Hive和Impala为例，当然在调研的过程中也会涉及到一些其他系统，如Spark SQL，Presto，TAJO等。而对于hawq这样的商业产品和apache drill这样成熟度还不是很高的开源方案就不做过多了解了。</p>
<h1 id="系统架构">系统架构</h1><h2 id="runtime_framework_v-s-_mpp">runtime framework v.s. mpp</h2><p>在SQL on Hadoop系统中，有两种架构，一种是基于某个运行时框架来构建查询引擎，典型案例是Hive；另一种是仿照过去关系数据库的MPP架构。前者现有运行时框架，然后套上sql层，后者则是从头打造一个一体化的查询引擎。有时我们能听到一种声音，说后者的架构优于前者，至少在性能上。那么是否果真如此？</p>
<p>一般来说，对于SQL on Hadoop系统很重要的一个评价指标就是：快。后面提到的所有内容也大多是为了查询速度更快。在Hive逐渐普及之后，就逐渐有了所谓交互式查询的需求，因为无论是BI系统，还是adhoc，都不能按照离线那种节奏玩。这时候无论是有实力的大公司（比如Facebook），还是专业的供应商（比如Cloudera），都试图去解决这个问题。短期可以靠商业方案或者关系数据库去支撑一下，但是长远的解决方案就是参考过去的MPP数据库架构打造一个专门的系统，于是就有了Impala，Presto等等。从任务执行的角度说，这类引擎的任务执行其实跟DAG模型是类似的，当时也有Spark这个DAG模型的计算框架了，但这终究是别人家的孩子，而且往Spark上套sql又是Hive的那种玩法了。于是在Impala问世之后就强调自己“计算全部在内存中完成”，性能也是各种碾压当时还只有MR作为计算模型的Hive。那么Hive所代表的“基于已有的计算模型”方式是否真的不行？</p>
<p>不可否认，按照这种方式去比较，那么类MPP模式确实有很多优势：</p>
<ul>
<li>DAG v.s. MR：最主要的优势，中间结果不写磁盘（除非内存不够），一气呵成。</li>
<li>流水线计算：上游stage一出结果马上推送或者拉到下一个stage处理，比如多表join时前两个表有结果直接给第三个表，不像MR要等两个表完全join完再给第三个表join。</li>
<li>高效的IO：本地查询没有多余的消耗，充分利用磁盘。这个后面细说。</li>
<li>线程级别的并发：相比之下MR每个task要启动JVM，本身就有很大延迟，占用资源也多。</li>
</ul>
<p>当然MPP模式也有其劣势，一个是扩展性不是很高，这在关系数据库时代就已经有过结论；另一个是容错性差，对于Impala来说一旦运行过程中出点问题，整个查询就挂了。</p>
<p>但是，经过不断的发展，Hive也能跑在DAG框架上了，不仅有Tez，还有Spark。上面提到的一些劣势，其实大都也可以在计算模型中解决，只不过考虑到计算模型的通用性和本身的设计目标，不会去专门满足（所以如果从这个角度分类，Impala属于“专用系统”，Spark则属于“通用系统”）。在最近Cloudera做的<a href="http://blog.Cloudera.com/blog/2014/09/new-benchmarks-for-sql-on-hadoop-Impala-1-4-widens-the-performance-gap/" target="_blank" rel="external">benchmark</a>中，虽然Impala仍然一路领先，但是基于Spark的Spark SQL完全不逊色于Presto，基于Tez的Hive也不算很差，至少在多用户并发模式下能超过Presto，足见MPP模式并不是绝对占上风的。所以这种架构上的区别在我看来并不是制胜的关键，至少不是唯一的因素，真正要做到快速查询，各个方面的细节都要有所把握。后面说的都是这些细节。</p>
<h2 id="核心组件">核心组件</h2><p>不管是上面提到的那种架构，一个SQL on Hadoop系统一般都会有一些通用的核心组件，这些组件根据设计者的考虑放在不同的节点角色中，在物理上节点都按照master/worker的方式去做，如果master压力太大，一些本来适合放在master上的组件可以放到一个辅助master上。</p>
<ul>
<li>UI层负责提供用户输入查询的接口。一般有Web/GUI，命令行，编程方式3类。</li>
<li>QL层负责把用户提交的查询解析成可以运行的执行计划（比如MR Job）。这部分在后面会专门提到。</li>
<li>执行层就是运行具体的Job。一般会有一个master负责query的运行管理，比如申请资源，观察进度等等，同时master也负责最终聚合局部结果到全局结果。而每个节点上会有相应的worker做本地计算。</li>
<li>IO层提供与存储层交互的接口。对于HDFS来说，需要根据I/O Format把文件转换成K/V，Serde再完成K/V到数据行的映射。对于非HDFS存储来说就需要一些专门的handler/connector。</li>
<li>存储层一般是HDFS，但也有可以查询NoSQL，或者关系数据库的。</li>
<li>系统另外还需要一个元数据管理服务，管理表结构等。</li>
</ul>
<p><img src="http://7xltg5.com1.z0.glb.clouddn.com/sql_on_hadoop_arch.jpg" alt="arch"></p>
<h1 id="执行计划">执行计划</h1><h2 id="编译流程">编译流程</h2><p>从SQL到执行计划，大致分为5步。</p>
<ul>
<li>第一步将SQL转换成抽象语法树AST。这一步一般都有第三方工具库可以完成，比如antlr。</li>
<li>第二步对AST进行语义分析，比如表是否存在，字段是否存在，SQL语义是否有误（比如select中被判定为聚合的字段在group by中有没有出现）。</li>
<li>第三步生成逻辑执行计划,这是一个由逻辑操作符组成的DAG。比如对于Hive来说扫表会产生TableScanOperator，聚合会产生GroupByOperator。对于类MPP系统来说，情况稍微有点不同。逻辑操作符的种类还是差不多，但是会先生成单机版本，然后生成多机版本。多机版本主要是把aggregate，join，还有top n这几个操作并行化，比如aggregate会分成类似MR那样的本地aggregate，shuffle和全局aggregate三步。</li>
<li>第四步做逻辑执行计划做优化，这步在下面单独介绍。</li>
<li>第五步把逻辑执行计划转换成可以在机器上运行的物理计划。对于Hive来说，就是MR/Tez Job等；对于Impala来说，就是plan fragment。其他类MPP系统也是类似的概念。物理计划中的一个计算单元（或者说Job），有“输入，处理，输出”三要素组成，而逻辑执行计划中的operator相对粒度更细，一个逻辑操作符一般处于这三要素之一的角色。</li>
</ul>
<p>下面分别举两个例子，直观的认识下sql、逻辑计划、物理计划之间的关系，具体解释各个operator的话会比较细碎，就不展开了。</p>
<h3 id="Hive_on_MR">Hive on MR</h3><figure class="highlight cs"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">select</span> <span class="title">count</span>(<span class="params"><span class="number">1</span></span>) <span class="keyword">from</span> status_updates <span class="keyword">where</span> ds </span>= <span class="string">'2009-08-01'</span></span><br></pre></td></tr></table></figure>
<p><img src="http://7xltg5.com1.z0.glb.clouddn.com/hive_compile.jpg" alt="Hive_compile"></p>
<h3 id="Presto">Presto</h3><p>引用自<a href="http://tech.meituan.com/presto.html" target="_blank" rel="external">美团技术团队</a>，其中SubPlan就是物理计划的一个计算单元</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">select</span> c1.<span class="keyword">rank</span>, <span class="keyword">count</span>(*) </span><br><span class="line"><span class="keyword">from</span> dim.city c1 <span class="keyword">join</span> dim.city c2 <span class="keyword">on</span> c1.<span class="keyword">id</span> = c2.<span class="keyword">id</span> </span><br><span class="line"><span class="keyword">where</span> c1.<span class="keyword">id</span> &gt; <span class="number">10</span> <span class="keyword">group</span> <span class="keyword">by</span> c1.<span class="keyword">rank</span> <span class="keyword">limit</span> <span class="number">10</span>;</span></span><br></pre></td></tr></table></figure>
<p><img src="http://7xltg5.com1.z0.glb.clouddn.com/presto_compile.png" alt="Presto_compile"></p>
<h2 id="优化器">优化器</h2><p>关于执行计划的优化，虽然不一定是整个编译流程中最难的部分，但却是最有看点的部分，而且目前还在不断发展中。Spark系之所以放弃Shark另起炉灶做Spark SQL，很大一部分原因是想自己做优化策略，避免受Hive的限制，为此还专门独立出优化器组件Catalyst（当然Spark SQL目前还是非常新，其未来发展给人不少想象空间）。总之这部分工作可以不断的创新，优化器越智能，越傻瓜化，用户就越能解放出来解决业务问题。</p>
<p>早期在Hive中只有一些简单的规则优化，比如谓词下推（把过滤条件尽可能的放在table scan之后就完成），操作合并（连续的filter用and合并成一个operator，连续的projection也可以合并）。后来逐渐增加了一些略复杂的规则，比如相同key的join + group by合并为1个MR，还有star schema join。在Hive 0.12引入的相关性优化（correlation optimizer）算是规则优化的一个高峰，他能够减少数据的重复扫描，具体来说，如果查询的两个部分用到了相同的数据，并且各自做group by / join的时候用到了相同的key，这个时候由于数据源和shuffle的key是一样的，所以可以把原来需要两个job分别处理的地方合成一个job处理。<br>比如下面这个sql：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">SELECT</span> </span><br><span class="line">	<span class="keyword">sum</span>(l_extendedprice) / <span class="number">7.0</span> <span class="keyword">as</span> avg_yearly </span><br><span class="line"><span class="keyword">FROM</span> </span><br><span class="line">	 (<span class="keyword">SELECT</span> l_partkey, l_quantity, l_extendedprice </span><br><span class="line">      <span class="keyword">FROM</span> lineitem <span class="keyword">JOIN</span> part <span class="keyword">ON</span> (p_partkey=l_partkey) </span><br><span class="line">      <span class="keyword">WHERE</span> p_brand=<span class="string">'Brand#35'</span> <span class="keyword">AND</span> p_container = <span class="string">'MED PKG'</span>)touter </span><br><span class="line"><span class="keyword">JOIN</span> </span><br><span class="line">     (<span class="keyword">SELECT</span> l_partkey <span class="keyword">as</span> lp, <span class="number">0.2</span> * <span class="keyword">avg</span>(l_quantity) <span class="keyword">as</span> lq </span><br><span class="line">      <span class="keyword">FROM</span> lineitem <span class="keyword">GROUP</span> <span class="keyword">BY</span> l_partkey) tinner </span><br><span class="line"><span class="keyword">ON</span> (touter.l_partkey = tinnter.lp) </span><br><span class="line"><span class="keyword">WHERE</span> touter.l_quantity &lt; tinner.lq</span></span><br></pre></td></tr></table></figure>
<p>这个查询中两次出现lineitem表，group by和两处join用的都是l_partkey，所以本来两个子查询和一个join用到三个job，现在只需要用到一个job就可以完成。</p>
<p><img src="http://7xltg5.com1.z0.glb.clouddn.com/correlation_optimizer.jpg" alt="correlation_optimizer"></p>
<p>但是，基于规则的优化（RBO）不能解决所有问题。在关系数据库中早有另一种优化方式，也就是基于代价的优化CBO。CBO通过收集表的数据信息（比如字段的基数，数据分布直方图等等）来对一些问题作出解答，其中最主要的问题就是确定多表join的顺序。CBO通过搜索join顺序的所有解空间（表太多的情况下可以用有限深度的贪婪算法），并且算出对应的代价，可以找到最好的顺序。这些都已经在关系数据库中得到了实践。</p>
<p>目前Hive已经启动专门的项目，也就是Apache Optiq来做这个事情，而其他系统也没有做的很好的CBO，所以这块内容还有很大的进步空间。</p>
<h1 id="执行效率">执行效率</h1><p>即使有了高效的执行计划，如果在运行过程本身效率较低，那么再好的执行计划也会大打折扣。这里主要关注CPU和IO方面的执行效率。</p>
<h2 id="CPU">CPU</h2><p>在具体的计算执行过程中，低效的cpu会导致系统的瓶颈落在CPU上，导致IO无法充分利用。在一项针对Impala和Hive的<a href="http://pages.cs.wisc.edu/~floratou/SQLOnHadoop.pdf" target="_blank" rel="external">对比</a>时发现，Hive在某些简单查询上（TPC-H Query 1）也比Impala慢主要是因为Hive运行时完全处于CPU bound的状态中，磁盘IO只有20%，而Impala的IO至少在85%。</p>
<p>在SQL on Hadoop中出现CPU bound的主要原因有以下几种：</p>
<ul>
<li>大量虚函数调用：这个问题在多处出现，比如对于<code>a + 2 * b</code>之类的表达式计算，解释器会构造一个expression tree，解释的过程就是递归调用子节点做evaluation的过程。又比如以DAG形式的operator/task在执行的过程中，上游节点会层层调用下游节点来获取产生的数据。这些都会产生大量的调用。</li>
<li>类型装箱：由于表达式解释器需要对不同数据类型的变量做解释，所以在Java中需要把这些本来是primitive的变量包装成Object，累积起来也消耗不少资源。这算是上面一个问题附带出来的。</li>
<li>branch instruction： 现在的CPU都是有并行流水线的，但是如果出现条件判断会导致无法并行。这种情况可能出现在判断数据的类型（是string还是int），或者在判断某一列是否因为其他字段的过滤条件导致本行不需要被读取（列存储情况下）。</li>
<li>cache miss：每次处理一行数据的方式导致cpu cache命中率不高。（这么说已经暗示了解决方案）</li>
</ul>
<p>针对上面的问题，目前大多数系统中已经加入了以下两个解决办法中至少一个。</p>
<p>一个方法是动态代码生成，也就是不使用解释性的统一代码。比如<code>a + 2 * b</code>这个表达式就会生成对应的执行语言的代码，而且可以直接用primitive type，而不是用固定的解释性代码。具体实现来说，JVM系的如Spark SQL，Presto可以用反射，C++系的Impala则使用了llvm生成中间码。对于判断数据类型造成的分支判断，动态代码的效果可以消除这些类型判断，还可以展开循环，可以对比下面这段代码，左边是解释性代码，右边是动态生成代码。</p>
<p><img src="http://7xltg5.com1.z0.glb.clouddn.com/codegen.jpg" alt="codegen"></p>
<p>另一个方法是vectorization（向量化），基本思路是放弃每次处理一行的模式，改用每次处理一小批数据（比如1k行），当然前提条件是使用列存储格式。这样一来，这一小批连续的数据可以放进cache里面，cpu不仅减少了branch instruction，甚至可以用SIMD加快处理速度。具体的实现参考下面的代码，对一个long型的字段增加一个常量。通过把数据表示成数组，过滤条件也用selVec装进数组，形成了很紧凑的循环：</p>
<figure class="highlight nimrod"><table><tr><td class="code"><pre><span class="line">add(<span class="type">int</span> vecNum, long[] <span class="literal">result</span>, long[] col1, <span class="type">int</span>[] col2, <span class="type">int</span>[] selVec) </span><br><span class="line">&#123;   </span><br><span class="line">  <span class="keyword">if</span> (selVec == null)   </span><br><span class="line">     <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; vecNum; i++) </span><br><span class="line">         <span class="literal">result</span>[i] = col1[i] + col2[i];</span><br><span class="line">  <span class="keyword">else</span> </span><br><span class="line">     <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; vecNum; i++) </span><br><span class="line">     &#123;</span><br><span class="line">         <span class="type">int</span> selIdx = selVec[i];</span><br><span class="line">         <span class="literal">result</span>[selIdx] = col1[selIdx] + col2[selIdx];</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="IO">IO</h2><p>由于SQL on Hadoop存储数据都是在HDFS上，所以IO层的优化其实大多数都是HDFS的事情，各大查询引擎则提出需求去进行推动。要做到高效IO，一方面要低延迟，屏蔽不必要的消耗；另一方面要高吞吐，充分利用每一块磁盘。目前与这方面有关的特性有：</p>
<ul>
<li>short-circuit local reads：当发现读取的数据是本地数据时，不走DataNode（因为要走一次socket连接），而是用DFS Client直接读本地的block replica。HDFS参数是<code>dfs.client.read.shortcircuit</code>和<code>dfs.domain.socket.path</code>。</li>
<li>zero copy：避免数据在内核buffer和用户buffer之间反复copy，在早期的HDFS中已经有这个默认实现。</li>
<li>disk-aware scheduling：通过知道每个block所在磁盘，可以在调度cpu资源时让不同的cpu读不同的磁盘，避免查询内和查询间的IO竞争。HDFS参数是<code>dfs.datanode.hdfs-blocks-metadata.enabled</code>。</li>
</ul>
<h1 id="存储格式">存储格式</h1><p>对于分析类型的workload来说，最好的存储格式自然是列存储，这已经在关系数据库时代得到了证明。目前hadoop生态中有两大列存储格式，一个是由Hortonworks和Microsoft开发的ORCFile，另一个是由Cloudera和Twitter开发的Parquet。</p>
<p>ORCFile顾名思义，是在RCFile的基础之上改造的。RCFile虽然号称列存储，但是只是“按列存储”而已，将数据先划分成row group，然后row group内部按照列进行存储。这其中没有列存储的一些关键特性，而这些特性在以前的列式数据库中（比如我以前用过的<a href="http://www.vldb.org/pvldb/1/1454174.pdf" target="_blank" rel="external">Infobright</a>）早已用到。好在ORCFile已经弥补了这些特性，包括：</p>
<ul>
<li>块过滤与块统计：每一列按照固定行数或大小进一步切分，对于切分出来的每一个数据单元，预先计算好这些单元的min/max/sum/count/null值，min/max用于在过滤数据的时候直接跳过数据单元，而所有这些统计值则可以在做聚合操作的时候直接采用，而不必解开这个数据单元做进一步的计算。</li>
<li>更高效的编码方式：RCFile中没有标注每一列的类型，事实上当知道数据类型时，可以采取特定的编码方式，本身就能很大程度上进行数据的压缩。常见的针对列存储的编码方式有RLE（大量重复数据），字典（字符串），位图（数字且基数不大），级差（排序过的数据，比如日志中用户访问时间）等等。</li>
</ul>
<p>ORCFile的结构如下图，数据先按照默认256M分为row group，也叫strip。每个strip配一个index，存放每个数据单元（默认10000行）的min/max值用于过滤；数据按照上面提到的编码方式序列化成stream，然后再进行snappy或gz压缩。footer提供读取stream的位置信息，以及更多的统计值如sum/count等。尾部的file footer和post script提供全局信息，如每个strip的行数，各列数据类型，压缩参数等。</p>
<p><img src="http://7xltg5.com1.z0.glb.clouddn.com/orcfile.jpg" alt="orcfile"></p>
<p>Parquet的设计原理跟ORC类似，不过它有两个特点：</p>
<ul>
<li>通用性：相比ORCFile专门给Hive使用而言，Parquet不仅仅是给Impala使用，还可以给其他查询工具使用，如Hive、Pig，进一步还能对接avro/thrift/pb等序列化格式。</li>
<li>基于Dremel思想的嵌套格式存储：关系数据库设计模式中反对存储复杂格式（违反第一范式），但是现在的大数据计算不仅出现了这种需求（半结构化数据），也能够高效的实现存储和查询效率，在语法上也有相应的支持（各种UDF，Hive的lateral view等）。Google Dremel就在实现层面做出了范例，Parquet则完全仿照了Dremel。</li>
</ul>
<p>对嵌套格式做列存储的难点在于，存储时需要标记某个数据对应于哪一个存储结构，或者说是哪条记录，所以需要用数据清楚的进行标记。 在Dremel中提出用definition level和repetition level来进行标记。definition level指的是，这条记录在嵌套结构中所处于第几层，而repetition level指的是，这条记录相对上一条记录，在第几层重复。比如下图是一个二级嵌套数组。图中的e跟f在都属于第二层的重复记录（同一个level2），所以f的r值为2，而c跟d则是不同的level2，但属于同一个level1，所以d的r值为1。对于顶层而言（新的一个嵌套结构），r值就为0。</p>
<p><img src="http://7xltg5.com1.z0.glb.clouddn.com/parquet_nested.jpg" alt="parquet_nested"></p>
<p>但是仅仅这样还不够。上图说明了r值的作用，但是还没有说明d值的作用，因为按照字面解释，d值对于每一个字段都是可以根据schema得到的，那为什么还要从行记录级别标记？这是因为记录中会插入一些null值，这些null值代表着他们“可以存在”但是因为是repeated或者是optional所以没有值的情况，null值是用来占位的（或者说是“想象”出来的），所以他们的值需要单独计算。null的d值就是说这个结构往上追溯到哪一层（不包括平级）就不是null（不是想象）了。在dremel paper中有完整的例子，例子中country的第一个null在code = en所在的结构里面，那么language不是null（不考虑code，他跟country平级），他就是第二层；又比如country的第二个null在url = http://B 所在的结构里面，那么name不是null（不考虑url，因为他跟本来就是null的language平级），所以就是第一层。</p>
<p><img src="http://7xltg5.com1.z0.glb.clouddn.com/dremel_data.jpg" alt="dremel_data"><br><img src="http://7xltg5.com1.z0.glb.clouddn.com/dremel_representation.jpg" alt="dremel_representation"></p>
<p>通过这种方式，就对一个树状的嵌套格式完成了存储。在读取的时候可以通过构造一个状态机进行遍历。</p>
<p>有意思的是，虽然parquet支持嵌套格式，但是Impala还没有来得及像Hive那样增加array，map，struct等复杂格式，当然这项功能已经被列入roadmap了，相信不久就会出现。</p>
<p>在最近我们做的Impala2.0测试中，顺便测试了存储格式的影响。parquet相比sequencefile在压缩比上达到1:5，查询性能也相差5-10倍，足见列存储一项就给查询引擎带来的提升。</p>
<h1 id="资源控制">资源控制</h1><h2 id="运行时资源调整">运行时资源调整</h2><p>对于一个MR Job，reduce task的数量一直是需要人为估算的一个麻烦事，基于MR的Hive也只是根据数据源大小粗略的做估计，不考虑具体的Job逻辑。但是在之后的框架中考虑到了这个情况，增加了运行时调整资源分配的功能。Tez中引入了vertex manager，可以根据运行时收集到的数据智能的判断reduce动作需要的task。类似的功能在TAJO中也有提到，叫progressive query optimization，而且TAJO不仅能做到动态调整task数量，还能动态调整join顺序。</p>
<h2 id="资源集成">资源集成</h2><p>在Hadoop已经进入2.x的时代，所有想要得到广泛应用的SQL on Hadoop系统势必要能与YARN进行集成。虽然这是一个有利于资源合理利用的好事，但是由于加入了YARN这一层，却给系统的性能带来了一定的障碍，因为启动AppMaster和申请container也会占用不少时间，尤其是前者，而且container的供应如果时断时续，那么会极大的影响时效性。在Tez和Impala中对这些问题给出了相应的解决办法：</p>
<ul>
<li>AppMaster启动延迟的问题，采取long lived app master，AppMaster启动后长期驻守，而非像是MR那样one AM per Job。具体实现时，可以给fair scheduler或capacity scheduler配置的每个队列配上一个AM池，有一定量的AM为提交给这个队列的任务服务。</li>
<li>container供应的问题，在Tez中采取了container复用的方式，有点像jvm复用，即container用完以后不马上释放，等一段时间，实在是没合适的task来接班了再释放，这样不仅减少container断供的可能，而且可以把上一个task留下的结果cache住给下一个task复用，比如做map join；Impala则采取比较激进的方式，一次性等所有的container分配到位了才开始执行查询，这种方式也能让它的流水线式的计算不至于阻塞。</li>
</ul>
<h1 id="其他">其他</h1><p>到这里为止，已经从上到下顺了一遍各个层面用到的技术，当然SQL on Hadoop本身就相当复杂，涉及到方方面面，时间精力有限不可能一一去琢磨。比如其他一些具有技术复杂度的功能有：</p>
<ul>
<li>多数据源查询：Presto支持从mysql，cassandra，甚至kafka中去读取数据，这就大大减少了数据整合时间，不需要放到HDFS里才能查询。Impala和Hive也支持查询hbase。Spark SQL也在1.2版本开始支持External Datasource。国内也有类似的工作，如秒针改造Impala使之能查询postgres。</li>
<li>近似查询：count distinct（基数估计）一直是sql性能杀手之一，如果能接受一定误差的话可以采用近似算法。Impala中已经实现了近似算法（ndv），Presto则是请blinkDB合作完成。两者都是采用了HyperLogLog Counting。当然，不仅仅是count distinct可以使用近似算法，其他的如取中位数之类的也可以用。</li>
</ul>
<h1 id="结束语">结束语</h1><p>尽管现在相关系统已经很多，也经过了几年的发展，但是目前各家系统仍然在不断的进行完善，比如：</p>
<ul>
<li>增加分析函数，复杂数据类型，SQL语法集的扩展。 </li>
<li>对于已经成形的技术也在不断的改进，如列存储还可以增加更多的encoding方式。</li>
<li>甚至对于像CBO这样的领域，开源界拿出来的东西还算是刚刚起步，相比HAWQ中的ORCA这种商业系统提供的优化器还差的很多。</li>
</ul>
<p>毕竟相比已经比较成熟的关系数据库，分布式环境下需要解决的问题更多，未来一定还会出现很多精彩的技术实践，让我们在海量数据中更快更方便的查到想要的数据。</p>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2014/09/27/the-log阅读笔记/" itemprop="url">
                The Log阅读笔记
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2014-09-27T10:20:41+08:00" content="2014-09-27">
            2014-09-27
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/data-system/" itemprop="url" rel="index">
                  <span itemprop="name">data system</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2014/09/27/the-log阅读笔记/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2014/09/27/the-log阅读笔记/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><p>几个月之前好几次看到有人在推荐Jay Kreps（后文中称其为“作者”）的<a href="http://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying" target="_blank" rel="external">这篇文章</a>。最近做的项目用到了 kafka，顺便把这篇文章翻出来看一看。文章很长，所以写个笔记梳理一下，内容大体上按照原文的顺序，但是也做了一些调整以便符合自己的思路。</p>
<h1 id="Part_1：什么是log">Part 1：什么是log</h1><p>log（日志）的定义其实很简单，就是按时间顺序依次保存的一系列记录，其实就是记录了系统依次发生了什么，强调有序性。这样简单的一个概念，在数据系统中发挥了很大的作用，这个part余下的内容就说明了日志的基本用途。</p>
<p>早期在关系数据库中，就大量使用log做ACID和replication。数据库replication使用的日志有两种格式，一种是sql based，记录操作请求，也叫逻辑日志；另一种是row based，记录数据变化，也叫物理日志。</p>
<p>到了分布式系统大行其道的年代，日志也有类似用法。物理日志一般用在primary back up架构，有明确的Leader和Follower关系，只有Leader提供请求处理，然后处理结果变成日志重放到其他Follower节点。逻辑日志一般对应了state machine replication，节点之间通常是p2p架构，请求序列被抽象为日志，每条请求都会被发往各个节点进行处理。</p>
<p><img src="http://7xltg5.com1.z0.glb.clouddn.com/active_and_passive_arch.png" alt="active_and_passive_arch"></p>
<p>其次，分布式系统中的一致性算法（Consensus）也用到了日志，比如raft中就用了state machine replication。</p>
<p>最后，日志在业务层面也可以起到对一个表进行重做的功能。这里强调的是从业务角度设计日志表。比如账户表和账户变更日志表的关系：账户表是账户当前的状态，而账户变更日志表可以通过重放日志把账户状态切换到任意一个时刻。这在业务层面有一点开发经验就很好理解。</p>
<p>以上只是日志在分布式系统内部的一些用途，理论成分略强，后面3个部分就来看看日志在企业构建数据平台的时候起到什么作用。</p>
<h1 id="Part_2：数据集成">Part 2：数据集成</h1><p>数据集成的概念有很多不同的理解，而且有时候跟一些工具/名词联系在一起，比如ETL。所以这里先给数据集成下一个定义：</p>
<blockquote>
<p>  <strong>数据集成就是让公司所有的数据能顺畅的给到各个系统使用。</strong></p>
</blockquote>
<p>按照这种说法，那么关系数据库时代，批处理模式的ETL就是数据集成的代名词，而现在出现了各种数据系统，数据集成需要更加的灵活，形成一个稳定可靠的数据流（data flow，或者叫data pipeline，即数据管道）。数据集成是数据应用的基础，报表/数据分析/线上服务都离不开可靠的数据集成，但是很多公司在数据流都没有搞好的情况下开始大搞hadoop/报表/分析等等，殊不知如果数据流出了问题，hadoop也不过是一堆废品。</p>
<p>这几年日志数据规模比以往大了很多，各行各业都有大量日志数据，比以往的数据库里的规模多多了。而且需要提供服务的系统也变得很多，比如OLAP，Search，NoSQL，MR，Spark，Stream等等。这些都是数据集成的挑战。在这里作者先把解决方案抛了出来，就是把所有数据以日志的形式放到一个中心日志集群，需要数据的系统以订阅的方式获取。这个集群可以供应不同类型的系统使用，比如流式计算就需要一直读取，而hadoop这样的就按照最起码10分钟一次的速度去读取。所以，有的系统处理日志的及时性高，有的及时性低。在Linkedin，这个中心日志集群就是kafka。</p>
<p><img src="http://7xltg5.com1.z0.glb.clouddn.com/log_subscription.png" alt="log_subscription"></p>
<p>在kafka诞生之前，linkedin在数据集成方面也经历过各种困难。最初他们开发了一个databus，专门用于同步数据库产生的log（如果用数据库的概念里面讲，就叫CDC），后来当他们开始用hadoop的时候又做了单独的从数据源到hadoop的pipeline，再之后又构建过一个从hadoop到kvstore的pipeline。这样的pipeline多了之后，发现维护成本很高。而且他们进一步的认识到，即使这样，目前公司各个系统中的数据收集到hadoop的其实不多，会有很多遗漏，数据之间的关系也很混乱。</p>
<p><img src="http://7xltg5.com1.z0.glb.clouddn.com/datapipeline_complex.png" alt="datapipeline_complex"></p>
<p>这些原因导致了作者去做kafka，把数据的生产者和消费者隔离开，形成一个清晰的集成方案。</p>
<p><img src="http://7xltg5.com1.z0.glb.clouddn.com/datapipeline_simple.png" alt="datapipeline_simple"></p>
<p>上述的基于日志的数据集成方式使得整个架构变成了一个事件驱动架构。举个例子来说明这种架构的灵活性：一个linkedin上的招聘页面（job post）产生的数据可能就要跟很多系统打交道，比如hadoop离线处理、反爬虫系统、在线分析报表（招聘方看的分析报表页面）、推荐系统等等。如果使用事件驱动架构，那么只需要把这个页面产生的pv数据发送到日志系统中，让各个系统去按需获取。</p>
<p>这个方案虽然看上去很美好，但是前提是要能构建出一个可扩展的日志系统。传统的消息系统很难胜任，而很多人眼中的分布式日志系统相关的应用往往都很笨重（比如zookeeper只能用来做元数据管理，如果真的用来做分布式事务，类似paxos这样的一致性协议的性能是比较差的）。但是linkedin每天产生600亿日志都能用kafka来收集，足以见得这件事情是可以做到的，当然，需要一些独特的设计来提高性能。文章中提到了3个点：</p>
<blockquote>
<ul>
<li>partition的概念：一个topic有多个partition组成，每个partition自身是有序的，这样不同的consumer通过拉不同节点的partition数据提高速度。</li>
<li>批量读写提高吞吐：很多细节的处理都用了batch的思路，主要是把消息按照组来包装，这样发数据，落磁盘，收数据等等都是批量进行。</li>
<li>减少IO：数据在kafka中的表示是高效的二进制格式，而且采用了zerocopy。</li>
</ul>
</blockquote>
<p>更多的细节和其他方面的优化，可以进一步去看kafka的设计文档。</p>
<p>最后作者阐述了一下对ETL的看法。传统的ETL一般被理解成DW的一个组成部分，模式一般是批处理，首先抽取数据，然后做清洗/转换，再整合到数据仓库中。但是这其实可以当成两件独立的事情来看待，第一个是形成干净的增量数据，也就是抽取和清洗，第二个是把数据整合到数据仓库，这一步会有一些数据模型的改变，比如把数据映射成星型模型。而第一件事情其实跟DW没有必然关系，他其实是数据集成的工作。如果一直沿用传统的ETL思路，比如网站点击流日志落地到本地文件，然后加载到hadoop/DW，DW收集的数据也没有办法通过日志集群跟其他系统共享，直接就进DW了。所以其实只要改变思路，就能让DW也统一到这种基于统一日志集群的架构中。另外，这么做对ETL团队还有一个好处，就是可以通过定义日志集群数据源系统的写入格式把数据清洗的工作（也就是ETL中的第一件事情）交给数据源系统做，毕竟他们更加清楚数据应该做什么清洗，而数据仓库作为日志集群的consumer团队之一，只需要做ETL中的第二件事情即可。不过个人觉得这个只是理论上的好处，实际推行起来肯定困难重重，因为这不是技术问题，而是团队协作问题。</p>
<p><img src="http://7xltg5.com1.z0.glb.clouddn.com/pipeline_ownership.png" alt="pipeline_ownership"></p>
<h1 id="Part_3：日志与实时流式计算">Part 3：日志与实时流式计算</h1><p>日志作为一种数据流，肯定会跟流式计算联系到一起。</p>
<p>首先还是要厘清流式计算的概念。现在的流式处理系统（比如storm）都是实时的，但是实际上流式计算不一定非要实时计算，也可以把一整天的数据用流式计算的方式去实现。流式计算和批处理计算的区别是流式计算是持续性的计算，而批处理计算往往需要保证一个固定的时间窗口，在这个时间窗口里，所有的数据都要落地之后才能开始计算，或者可以说，批处理计算是静态的。流式计算的时间窗口的概念包含了批处理计算，但是批处理计算却做不到流式计算的低延迟。</p>
<p>以前实时数据流不太普及，所以流式计算也没有普及——即使有流式计算系统也没有用武之地，因为数据流不是实时的。但是现在kafka这样的系统保证了实时日志数据流，所以流式计算也自然可以有所施展。 </p>
<p>如果说上一部分的数据集成中，日志起到的作用是统一的入口和出口，那么在流式计算中日志则扩展成了数据流图（dataflow graph），也就是经过流式计算处理的日志变成了一个二手日志（derived log）。而且流式计算跟kafka会碰到一样的问题，就是consumer跟不上producer的速度，那么日志就起到了buffer的作用，否则要么丢数据，要么系统block住等consumer跟上进度。</p>
<p><img src="http://7xltg5.com1.z0.glb.clouddn.com/dag.png" alt="dag"></p>
<p>既然谈到了buffer的问题，作者顺便说了日志保存周期的事情，因为日志不可能无限制保存，导致无限制的增长。kafka对于过期日志有两种处理方式：对于纯时间序列型的事件类日志——比如网站访问记录，可以设置过期时间；而对于更新类日志——比如对于网站用户的信息更新日志，过期删除就不太合适，所以kafka提供了log compaction，也就是根据更新的key保存最新一个版本的数据。</p>
<p><img src="http://7xltg5.com1.z0.glb.clouddn.com/log_compaction_0.png" alt="log_compaction_0"></p>
<p>作者还顺带说了流式处理中的一个问题，就是对于聚合或关联等场景，需要保存中间状态，直接保存在内存会有丢失风险（机器宕机）。对于此问题，Storm的tridnet方案则是保存在统一的远程存储，比如cassandra（下左图）。 而Linkedin的流计算系统Smaza在这个问题上采取的方案是把状态保存到各自节点的本地存储，比如bdb（下右图）。相比之下，Storm会有性能，资源隔离性等多方面的问题。</p>
<p><img src="http://7xltg5.com1.z0.glb.clouddn.com/storm_vs_smaza.jpg" alt="storm_vs_smaza"></p>
<h1 id="Part_4：系统构建">Part 4：系统构建</h1><p>现在各大互联网公司都会有各种各样的数据系统，数不胜数，上面已经提到过了。这里说的系统构建，指的是把这些系统用一个统一的视角抽象的看成一个单一的分布式数据库。而各式各样的数据系统起到的作用，其实是这个数据库的各种index（针对查询类系统来说）和各种trigger与物化视图（针对流式计算系统来说）。</p>
<p>那么有没有可能确实能搞出一个单一的大系统呢？目前来说很难，因为需要处理各种各样的应用场景，很多场景的要求是相互矛盾，不可兼得。目前的各种系统基本上都是解决一部分应用场景。不过好在现在开源生态圈很繁荣，可以选取各种需要的工具进行组合，拼装成一个大的整体架构，也就是前面说的单一的分布式数据库。</p>
<p>从逻辑角度来说，日志在这个整体架构中的定位是这样的（下左图）：整个系统分成serving和log两层，用户的各种请求被写入日志，feed到serving层各种查询系统，最后用户查询的时候就达到了read your writes（最终一致性）的效果。在这个逻辑架构中，serving层需要做的只是制定用户api，还有构建查询需要的索引（上面已经提到过局部系统是整体系统的一个索引），比如搜索引擎的倒排索引，kv store的btree索引等等。而日志系统则解决了很多其他重要的问题：数据的一致性，多副本，数据均衡，外部数据源订阅，副本丢失恢复，commit语义（意思是通过kafka的保证数据不丢，因为在kafka设计中，消息commit之后才能comsumer）。</p>
<p><img src="http://7xltg5.com1.z0.glb.clouddn.com/log_based_system.jpg" alt="log_based_system"></p>
<p>可能有人会质疑用户请求写入日志浪费磁盘资源，但这恰恰符合kafka的设计理念：don’t fear the file system。因为这里有一个前提，就是日志是顺序读，性能不比内存随机读差，硬件方面也可以用大硬盘。而且文件系统读写时OS会有page cache，非常稳定，相比之下数据保存在内存的方案用的都是进程的内存，稳定性可能欠佳，比如jvm平台上会有GC问题。</p>
<p>Linkedin就是根据这个思路搭建的实际架构（上右图），从某种角度来说，serving层的各种系统只是日志流的一层cache而已。</p>
<h1 id="结语">结语</h1><p>作者在文末列举了很多的参考资料，如果想进一步了解，请移步至<a href="http://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying" target="_blank" rel="external">原文</a>查看。</p>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2014/03/06/mac-osx上安装hadoop2/" itemprop="url">
                Mac OSX上安装Hadoop2
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2014-03-06T16:57:07+08:00" content="2014-03-06">
            2014-03-06
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/data-system/" itemprop="url" rel="index">
                  <span itemprop="name">data system</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2014/03/06/mac-osx上安装hadoop2/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2014/03/06/mac-osx上安装hadoop2/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><p>最近忙里偷闲，在刚刚到手的mac上安装了hadoop2.3。由于时间仓促，个别组件甚至没有来得及跑起来，匆匆的记一下。</p>
<h1 id="设置SSH">设置SSH</h1><p>单节点运行hadoop需要本机ssh连通。生成key的过程没什么特殊的：</p>
<figure class="highlight ruby"><table><tr><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br><span class="line">cat .ssh/id_rsa.pub <span class="prompt">&gt;&gt;</span>.ssh/authorized_keys</span><br></pre></td></tr></table></figure>
<p>然后需要开启ssh服务，具体设置在“系统偏好设置-&gt;共享-&gt;远程登录”，把这个勾上即可。</p>
<p>如果发现配置之后不生效，多半是权限问题，可以检查一下<code>authorized_keys</code>是不是权限是600，另外家目录（比如对我的环境来讲就是<code>/Users/sunyi</code>）的权限是不是755。</p>
<h1 id="环境变量">环境变量</h1><p>首先保证安装了java7，然后到官网下载hadoop2.3版本，解压（方便起见就不新开用户了，直接放到当前用户目录下）。接下来就需要做些环境变量设置。</p>
<h2 id="hadoop环境变量">hadoop环境变量</h2><p>在/etc/bashrc文件中增加如下设置：</p>
<figure class="highlight xquery"><table><tr><td class="code"><pre><span class="line">export HADOOP_HOME=/Users/sunyi/hadoop-<span class="number">2.3</span>.<span class="number">0</span></span><br><span class="line">export PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP</span>_HOME/bin:<span class="variable">$HADOOP</span>_HOME/sbin</span><br></pre></td></tr></table></figure>
<p>这样就能直接访问hadoop shell了，可以用如下命令做个测试:</p>
<figure class="highlight applescript"><table><tr><td class="code"><pre><span class="line">hadoop <span class="property">version</span></span><br></pre></td></tr></table></figure>
<h2 id="Java环境变量">Java环境变量</h2><p>在<code>hadoop-env.sh</code>/<code>mapred-env.sh</code>/<code>yarn-env.sh</code>（位于 ${HADOOP_HOME}/ etc/ hadoop/）里面都设一下，根据<a href="http://han.guokai.blog.163.com/blog/static/136718271201301183938165/" target="_blank" rel="external">这里</a>的建议做了如下设置：</p>
<figure class="highlight autohotkey"><table><tr><td class="code"><pre><span class="line">export JAV<span class="built_in">A_HOME</span>=<span class="escape">`/</span>usr/libexec/jav<span class="built_in">a_home</span>`</span><br></pre></td></tr></table></figure>
<p>我在配置的时候，一开始漏做了java环境变量，结果启动hadoop后跑job就报<a href="http://stackoverflow.com/questions/21275735/can-not-run-testdistributedshell-in-eclipse" target="_blank" rel="external">这个错误</a>。</p>
<h1 id="Hadoop配置项">Hadoop配置项</h1><p>这里就直接罗列一下配置项了。</p>
<p><strong>core-site.xml</strong></p>
<figure class="highlight pf"><table><tr><td class="code"><pre><span class="line"><span class="variable">&lt;configuration&gt;</span>  </span><br><span class="line">   <span class="variable">&lt;property&gt;</span>  </span><br><span class="line">     <span class="variable">&lt;name&gt;</span>fs.<span class="keyword">default</span>.name<span class="variable">&lt;/name&gt;</span>  </span><br><span class="line">       <span class="variable">&lt;value&gt;</span>hdfs://localhost:<span class="number">9000</span><span class="variable">&lt;/value&gt;</span>  </span><br><span class="line">   <span class="variable">&lt;/property&gt;</span>  </span><br><span class="line"> <span class="variable">&lt;/configuration&gt;</span></span><br></pre></td></tr></table></figure>
<p><strong>hdfs-site.xml</strong></p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">configuration</span>&gt;</span>  </span><br><span class="line">   <span class="tag">&lt;<span class="title">property</span>&gt;</span>  </span><br><span class="line">     <span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="title">name</span>&gt;</span>  </span><br><span class="line">     <span class="tag">&lt;<span class="title">value</span>&gt;</span>1<span class="tag">&lt;/<span class="title">value</span>&gt;</span>  </span><br><span class="line">   <span class="tag">&lt;/<span class="title">property</span>&gt;</span>  </span><br><span class="line">   <span class="tag">&lt;<span class="title">property</span>&gt;</span>  </span><br><span class="line">     <span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="title">name</span>&gt;</span>  </span><br><span class="line">     <span class="tag">&lt;<span class="title">value</span>&gt;</span>file:/Users/sunyi/hdfs/namenode<span class="tag">&lt;/<span class="title">value</span>&gt;</span>  </span><br><span class="line">   <span class="tag">&lt;/<span class="title">property</span>&gt;</span>  </span><br><span class="line">   <span class="tag">&lt;<span class="title">property</span>&gt;</span>  </span><br><span class="line">     <span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="title">name</span>&gt;</span>  </span><br><span class="line">     <span class="tag">&lt;<span class="title">value</span>&gt;</span>file:/Users/sunyi/hdfs/datanode<span class="tag">&lt;/<span class="title">value</span>&gt;</span>  </span><br><span class="line">   <span class="tag">&lt;/<span class="title">property</span>&gt;</span>  </span><br><span class="line"><span class="tag">&lt;/<span class="title">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p><strong>mapred-site.xml</strong></p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="title">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="title">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p><strong>yarn-site.xml</strong></p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">configuration</span>&gt;</span>  </span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="title">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="title">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="title">name</span>&gt;</span>yarn.nodemanager.aux-services.mapreduce_shuffle.class<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="title">value</span>&gt;</span>org.apache.hadoop.mapred.ShuffleHandler<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="title">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>其中<code>hdfs-site.xml</code>注意提前建好相应的目录，<code>yarn-site.xml</code>配置时注意不要用以前2.0版本时的配置，也就是不要用mapreduce.shuffle或者mapreduce-shuffle这样的写法，如果这样写的话，会出现启动后nodemanager启动失败，还有job运行失败等现象。</p>
<h1 id="启动Hadoop">启动Hadoop</h1><p>首先格式化hdfs，然后依次启动hdfs和yarn。</p>
<figure class="highlight dos"><table><tr><td class="code"><pre><span class="line">hadoop namenode -<span class="built_in">format</span></span><br><span class="line">./sbin/<span class="built_in">start</span>-dfs.sh</span><br><span class="line">./sbin/<span class="built_in">start</span>-yarn.sh</span><br></pre></td></tr></table></figure>
<p>然后可以用jps确认下进程全部顺利启动，会看到下面这些进程，有问题就检查下log。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="number">8431</span> ResourceManager</span><br><span class="line"><span class="number">8525</span> NodeManager</span><br><span class="line"><span class="number">9095</span> Jps</span><br><span class="line"><span class="number">8313</span> SecondaryNameNode</span><br><span class="line"><span class="number">8207</span> DataNode</span><br><span class="line"><span class="number">8120</span> NameNode</span><br></pre></td></tr></table></figure>
<p>成功启动后，就可以用浏览器看看<code>localhost:50070</code>和<code>localhost:8088</code>的hdfs和yarn管理页面了。</p>
<h1 id="Job测试">Job测试</h1><p>找一个文本，传到hdfs上面，然后运行一下hadoop自带的wordcount job：</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-*<span class="class">.jar</span> wordcount <span class="tag">input</span><span class="class">.txt</span> out</span><br></pre></td></tr></table></figure>
<p>或者运行一个不依赖hdfs数据的job也可以，比如pi计算：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar pi <span class="number">10</span> <span class="number">100000</span></span><br></pre></td></tr></table></figure>
<p>运行成功后yarn管理页如下图：<br> <img src="http://7xltg5.com1.z0.glb.clouddn.com/yarn_webui.jpg" alt="yarn_webui"><br>到此为止，就算安装完成了。至于开头说到的没有跑起来的组件，是hadoop 2中单独分离出来的job history server。等以后有时间了再来配置。</p>
<h1 id="日志警告">日志警告</h1><p>在运行job或者是其他任何hadoop命令时，都会碰到下面两个warning。不处理也没关系，不影响学习使用：</p>
<ul>
<li><p><strong>Unable to load realm info from SCDynamicStore</strong>：这个是mac下特有的安全方面的问题警告，可以通过在<code>hadoop-env.sh</code>中增加如下配置消除。</p>
<figure class="highlight 1c"><table><tr><td class="code"><pre><span class="line">HADOOP_OPTS=<span class="string">"$&#123;HADOOP_OPTS&#125; -Djava.security.krb5.realm= </span></span><br><span class="line">-Djava.security.krb5.kdc= -Djava.security.krb5.conf=/dev/null<span class="string">"</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Unable to load native-hadoop library for your platform</strong>：这个警告的原因是hadoop自带的native lib是32位的，在64位的系统上自然是版本不一致。如果需要解决这个问题，需要自己编译一下，替换掉原来的lib即可，由于这里仅仅是学习用，就不做这件事情了。</p>
</li>
</ul>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2013/09/01/数据仓库中的sql性能优化（hive篇）/" itemprop="url">
                数据仓库中的SQL性能优化（Hive篇）
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2013-09-01T18:01:52+08:00" content="2013-09-01">
            2013-09-01
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/data-system/" itemprop="url" rel="index">
                  <span itemprop="name">data system</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2013/09/01/数据仓库中的sql性能优化（hive篇）/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2013/09/01/数据仓库中的sql性能优化（hive篇）/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><p>一个Hive查询生成多个map reduce job，一个map reduce job又有map，reduce，spill，shuffle，sort等多个阶段，所以针对hive查询的优化可以大致分为针对MR中单个步骤的优化（其中又会有细分），针对MR全局的优化，和针对整个查询（多MR job）的优化，下文会分别阐述。</p>
<p><img src="http://7xltg5.com1.z0.glb.clouddn.com/hive_opt_section.jpg" alt="hive_opt_section"></p>
<p>在开始之前，先把MR的流程图帖出来（摘自Hadoop权威指南），方便后面对照。另外要说明的是，这个优化只是针对Hive 0.9版本，而不是后来Hortonwork发起Stinger项目之后的版本。相对应的Hadoop版本是1.x而非2.x。</p>
<p><img src="http://7xltg5.com1.z0.glb.clouddn.com/mr.jpg" alt="mr process"></p>
<h1 id="Map阶段的优化(map_phase)">Map阶段的优化(map phase)</h1><p>Map阶段的优化，主要是确定合适的map数。那么首先要了解map数的计算公式：</p>
<figure class="highlight processing"><table><tr><td class="code"><pre><span class="line">num_map_tasks = <span class="built_in">max</span>[$&#123;mapred.<span class="built_in">min</span>.<span class="built_in">split</span>.<span class="built_in">size</span>&#125;,</span><br><span class="line">                <span class="built_in">min</span>($&#123;dfs.block.<span class="built_in">size</span>&#125;, $&#123;mapred.<span class="built_in">max</span>.<span class="built_in">split</span>.<span class="built_in">size</span>&#125;)]</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li><code>mapred.min.split.size</code>指的是数据的最小分割单元大小。  </li>
<li><code>mapred.max.split.size</code>指的是数据的最大分割单元大小。  </li>
<li><code>dfs.block.size</code>指的是HDFS设置的数据块大小。  </li>
</ul>
</blockquote>
<p>一般来说<code>dfs.block.size</code>这个值是一个已经指定好的值，而且这个参数hive是识别不到的：</p>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line"><span class="tag">hive</span>&gt; <span class="tag">set</span> <span class="tag">dfs</span><span class="class">.block</span><span class="class">.size</span>;</span><br><span class="line"><span class="tag">dfs</span><span class="class">.block</span><span class="class">.size</span> <span class="tag">is</span> <span class="tag">undefined</span></span><br></pre></td></tr></table></figure>
<p>所以实际上只有<code>mapred.min.split.size</code>和<code>mapred.max.split.size</code>这两个参数（本节内容后面就以min和max指代这两个参数）来决定map数量。在hive中min的默认值是1B，max的默认值是256MB：</p>
<figure class="highlight processing"><table><tr><td class="code"><pre><span class="line">hive&gt; <span class="built_in">set</span> mapred.<span class="built_in">min</span>.<span class="built_in">split</span>.<span class="built_in">size</span>;</span><br><span class="line">mapred.<span class="built_in">min</span>.<span class="built_in">split</span>.<span class="built_in">size</span>=<span class="number">1</span></span><br><span class="line">hive&gt; <span class="built_in">set</span> mapred.<span class="built_in">max</span>.<span class="built_in">split</span>.<span class="built_in">size</span>;</span><br><span class="line">mapred.<span class="built_in">max</span>.<span class="built_in">split</span>.<span class="built_in">size</span>=<span class="number">256000000</span></span><br></pre></td></tr></table></figure>
<p>所以如果不做修改的话，就是1个map task处理256MB数据，我们就以调整max为主。通过调整max可以起到调整map数的作用，减小max可以增加map数，增大max可以减少map数。需要提醒的是，直接调整<code>mapred.map.tasks</code>这个参数是没有效果的。</p>
<p>调整大小的时机根据查询的不同而不同，总的来讲可以通过观察map task的完成时间来确定是否需要增加map资源。如果map task的完成时间都是接近1分钟，甚至几分钟了，那么往往增加map数量，使得每个map task处理的数据量减少，能够让map task更快完成；而如果map task的运行时间已经很少了，比如10-20秒，这个时候增加map不太可能让map task更快完成，反而可能因为map需要的初始化时间反而让job总体速度变慢，这个时候反而需要考虑是否可以把map的数量减少，这样可以节省更多资源给其他Job。</p>
<h1 id="Reduce阶段的优化(reduce_phase)">Reduce阶段的优化(reduce phase)</h1><p>这里说的reduce阶段，是指前面流程图中的reduce phase（实际的reduce计算）而非图中整个reduce task。Reduce阶段优化的主要工作也是选择合适的reduce task数量，跟上面的map优化类似。<br>与map优化不同的是，reduce优化时，可以直接设置<code>mapred.reduce.tasks</code>参数从而直接指定reduce的个数。当然直接指定reduce个数虽然比较方便，但是不利于自动扩展。Reduce数的设置虽然相较map更灵活，但是也可以像map一样设定一个自动生成规则，这样运行定时job的时候就不用担心原来设置的固定reduce数会由于数据量的变化而不合适。</p>
<p>Hive估算reduce数量的时候，使用的是下面的公式：</p>
<figure class="highlight mel"><table><tr><td class="code"><pre><span class="line">num_reduce_tasks = <span class="keyword">min</span>[$&#123;hive.<span class="keyword">exec</span>.reducers.<span class="keyword">max</span>&#125;, </span><br><span class="line">                      ($&#123;input.<span class="keyword">size</span>&#125; / $&#123; hive.<span class="keyword">exec</span>.reducers.bytes.per.reducer&#125;)]</span><br></pre></td></tr></table></figure>
<p><code>hive.exec.reducers.bytes.per.reducer</code>默认为1G，也就是每个reduce处理相当于job输入文件中1G大小的对应数据量，而且reduce个数不能超过一个上限参数值，这个参数的默认取值为999。所以我们也可以用调整这个公式的方式调整reduce数量，在灵活性和定制性上取得一个平衡。</p>
<p>设置reduce数同样也是根据运行时间作为参考调整，并且可以根据特定的业务需求、工作负载类型总结出经验，所以不再赘述。</p>
<h1 id="Map与Reduce之间的优化(spill,_copy,_sort_phase)">Map与Reduce之间的优化(spill, copy, sort phase)</h1><p>map phase和reduce phase之间主要有3道工序。首先要把map输出的结果进行排序后做成中间文件，其次这个中间文件就能分发到各个reduce，最后reduce端在执行reduce phase之前把收集到的排序子文件合并成一个排序文件。这个部分可以调的参数挺多，但是一般都是不要调整的，不必重点关注。</p>
<h2 id="Spill_与_Sort">Spill 与 Sort</h2><p>在spill阶段，由于内存不够，数据可能没办法在内存中一次性排序完成，那么就只能把局部排序的文件先保存到磁盘上，这个动作叫spill，然后spill出来的多个文件可以在最后进行merge。如果发生spill，可以通过设置<code>io.sort.mb</code>来增大mapper输出buffer的大小，避免spill的发生。另外合并时可以通过设置<code>io.sort.factor</code>来使得一次性能够合并更多的数据。调试参数的时候，一个要看spill的时间成本，一个要看merge的时间成本，还需要注意不要撑爆内存（<code>io.sort.mb</code>是算在map的内存里面的）。Reduce端的merge也是一样可以用<code>io.sort.factor</code>。一般情况下这两个参数很少需要调整，除非很明确知道这个地方是瓶颈。</p>
<h2 id="Copy">Copy</h2><p>copy阶段是把文件从map端copy到reduce端。默认情况下在5%的map完成的情况下reduce就开始启动copy，这个有时候是很浪费资源的，因为reduce一旦启动就被占用，一直等到map全部完成，收集到所有数据才可以进行后面的动作，所以我们可以等比较多的map完成之后再启动reduce流程，这个比例可以通<code>mapred.reduce.slowstart.completed.maps</code>去调整，他的默认值就是5%。如果觉得这么做会减慢reduce端copy的进度，可以把copy过程的线程增大。<code>tasktracker.http.threads</code>可以决定作为server端的map用于提供数据传输服务的线程，<code>mapred.reduce.parallel.copies</code>可以决定作为client端的reduce同时从map端拉取数据的并行度（一次同时从多少个map拉数据），修改参数的时候这两个注意协调一下，server端能处理client端的请求即可。</p>
<h1 id="文件格式的优化">文件格式的优化</h1><p>文件格式方面有两个问题，一个是给输入和输出选择合适的文件格式，另一个则是小文件问题。小文件问题在目前的hive环境下已经得到了比较好的解决，hive的默认配置中就可以在小文件输入时自动把多个文件合并给1个map处理，输出时如果文件很小也会进行一轮单独的合并，所以这里就不专门讨论了。相关的参数可以在<a href="http://blog.csdn.net/yfkiss/article/details/8590486" target="_blank" rel="external">这里</a>找到。</p>
<p>关于文件格式，Hive0.9版本有3种，textfile，sequencefile和rcfile。总体上来说，rcfile的压缩比例和查询时间稍好一点，所以推荐使用。</p>
<p>关于使用方法，可以在建表结构时可以指定格式，然后指定压缩插入：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">create</span> <span class="keyword">table</span> rc_file_test( <span class="keyword">col</span> <span class="built_in">int</span> ) <span class="keyword">stored</span> <span class="keyword">as</span> rcfile;</span></span><br><span class="line"><span class="operator"><span class="keyword">set</span> hive.exec.<span class="keyword">compress</span>.<span class="keyword">output</span> = <span class="literal">true</span>;</span></span><br><span class="line"><span class="operator"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> rc_file_test</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> source_table;</span></span><br></pre></td></tr></table></figure>
<p>另外时也可以指定输出格式，也可以通过<code>hive.default.fileformat</code>来设定输出格式，适用于create table as select的情况：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">set</span> hive.<span class="keyword">default</span>.fileformat = SequenceFile;</span></span><br><span class="line"><span class="operator"><span class="keyword">set</span> hive.exec.<span class="keyword">compress</span>.<span class="keyword">output</span> = <span class="literal">true</span>;</span> </span><br><span class="line"><span class="comment">/*对于sequencefile，有record和block两种压缩方式可选，block压缩比更高*/</span></span><br><span class="line"><span class="operator"><span class="keyword">set</span> mapred.<span class="keyword">output</span>.compression.<span class="keyword">type</span> = <span class="keyword">BLOCK</span>;</span> </span><br><span class="line"><span class="operator"><span class="keyword">create</span> <span class="keyword">table</span> seq_file_test</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> source_table;</span></span><br></pre></td></tr></table></figure>
<p>上面的文件格式转换，其实是由hive完成的（也就是插入动作）。但是也可以由外部直接导入纯文本（可以按照<a href="http://slaytanic.blog.51cto.com/2057708/1162287" target="_blank" rel="external">这里</a>的做法预先压缩），或者是由MapReduce Job生成的数据。</p>
<p>值得注意的是，hive读取sequencefile的时候，是把key忽略的，也就是直接读value并且按照指定分隔符分隔字段。但是如果hive的数据来源是从mr生成的，那么写sequencefile的时候，key和value都是有意义的，key不能被忽略，而是应该当成第一个字段。为了解决这种不匹配的情况，有两种办法。一种是要求凡是结果会给hive用的mr job输出value的时候带上key。但是这样的话对于开发是一个负担，读写数据的时候都要注意这个情况。所以更好的方法是第二种，也就是把这个源自于hive的问题交给hive解决，写一个InputFormat包装一下，把value输出加上key即可。以下是核心代码，修改了RecordReader的next方法：</p>
<figure class="highlight processing"><table><tr><td class="code"><pre><span class="line"><span class="comment">//注意：这里为了简化，假定了key和value都是Text类型，所以MR的输出的k/v都要是Text类型。</span></span><br><span class="line"><span class="comment">//这个简化还会造成数据为空时，出现org.apache.hadoop.io.BytesWritable cannot be cast to org.apache.hadoop.io.Text的错误，因为默认hive的sequencefile的key是一个空的ByteWritable。</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="built_in">boolean</span> next(K <span class="variable">key</span>, V value) <span class="keyword">throws</span> IOException </span><br><span class="line">&#123;</span><br><span class="line">    Text tKey = (Text) <span class="variable">key</span>;</span><br><span class="line">    Text tValue = (Text) value;</span><br><span class="line">    <span class="keyword">if</span> (!<span class="keyword">super</span>.next(innerKey, innerValue)) </span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line"></span><br><span class="line">    Text inner_key = (Text) innerKey; <span class="comment">//在构造函数中用createKey()生成</span></span><br><span class="line">    Text inner_value = (Text) innerValue; <span class="comment">//在构造函数中用createValue()生成</span></span><br><span class="line"></span><br><span class="line">    tKey.<span class="built_in">set</span>(inner_key);</span><br><span class="line">    tValue.<span class="built_in">set</span>(inner_key.toString() + <span class="string">'\t'</span> + inner_value.toString()); <span class="comment">// 分隔符注意自己定义</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="Job整体优化">Job整体优化</h1><p>有一些问题必须从job的整体角度去观察。这里讨论几个问题：Job执行模式（本地执行v.s.分布式执行）、JVM重用、索引、Join算法、数据倾斜。</p>
<h2 id="Job执行模式">Job执行模式</h2><p>Hadoop的map reduce job可以有3种模式执行，即本地模式，伪分布式，还有真正的分布式。本地模式和伪分布式都是在最初学习hadoop的时候往往被说成是做单机开发的时候用到。但是实际上对于处理数据量非常小的job，直接启动分布式job会消耗大量资源，而真正执行计算的时间反而非常少。这个时候就应该使用本地模式执行mr job，这样执行的时候不会启动分布式job，执行速度就会快很多。比如一般来说启动分布式job，无论多小的数据量，执行时间一般不会少于20s，而使用本地mr模式，10秒左右就能出结果。</p>
<p>设置执行模式的主要参数有三个，一个是<code>hive.exec.mode.local.auto</code>，把他设为true就能够自动开启local mr模式。但是这还不足以启动local mr，输入的文件数量和数据量大小必须要控制，这两个参数分别为<code>hive.exec.mode.local.auto.tasks.max</code>和<code>hive.exec.mode.local.auto.inputbytes.max</code>，默认值分别为4和128MB，即默认情况下，map处理的文件数不超过4个并且总大小小于128MB就启用local mr模式。</p>
<p>另外，如果是简单的select语句，比如select某个列取个10条数据看看sample，那么在hive0.10之后有专门的fetch task优化，使用参数<code>hive.fetch.task.conversion</code>即可。</p>
<h2 id="JVM重用">JVM重用</h2><p>正常情况下，MapReduce启动的JVM在完成一个task之后就退出了，但是如果任务花费时间很短，又要多次启动JVM的情况下（比如对很大数据量进行计数操作），JVM的启动时间就会变成一个比较大的overhead。在这种情况下，可以使用jvm重用的参数：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">set</span> mapred.job.<span class="keyword">reuse</span>.jvm.<span class="keyword">num</span>.tasks = <span class="number">5</span>;</span></span><br></pre></td></tr></table></figure>
<p>他的作用是让一个jvm运行多次任务之后再退出。这样一来也能节约不少JVM启动时间。</p>
<h2 id="索引">索引</h2><p>总体上来说，hive的索引目前还是一个不太适合使用的东西，这里只是考虑到叙述完整性，对其进行基本的介绍。</p>
<p>Hive中的索引架构开放了一个接口，允许你根据这个接口去实现自己的索引。目前hive自己有一个参考的索引实现（CompactIndex），后来在0.8版本中又加入位图索引。这里就讲讲CompactIndex。</p>
<p>CompactIndex的实现原理类似一个lookup table，而非传统数据库中的B树。如果你对table A的col1做了索引，索引文件本身就是一个table，这个table会有3列，分别是col1的枚举值，每个值对应的数据文件位置，以及在这个文件位置中的偏移量。通过这种方式，可以减少你查询的数据量（偏移量可以告诉你从哪个位置开始找，自然只需要定位到相应的block），起到减少资源消耗的作用。但是就其性能来说，并没有很大的改善，很可能还不如构建索引需要花的时间。所以在集群资源充足的情况下，没有太大必要考虑索引。</p>
<p>CompactIndex的还有一个缺点就是使用起来不友好，索引建完之后，使用之前还需要根据查询条件做一个同样剪裁才能使用，索引的内部结构完全暴露，而且还要花费额外的时间。具体看看下面的使用方法就了解了：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*在index_test_table表的id字段上创建索引*/</span></span><br><span class="line"><span class="operator"><span class="keyword">create</span> <span class="keyword">index</span> idx <span class="keyword">on</span> <span class="keyword">table</span> index_test_table(<span class="keyword">id</span>)  </span><br><span class="line"><span class="keyword">as</span> <span class="string">'org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler'</span> <span class="keyword">with</span> <span class="keyword">deferred</span> <span class="keyword">rebuild</span>;</span></span><br><span class="line"><span class="operator"><span class="keyword">alter</span> <span class="keyword">index</span> idx <span class="keyword">on</span> index_test_table <span class="keyword">rebuild</span>;</span></span><br><span class="line">	</span><br><span class="line"><span class="comment">/*索引的剪裁。找到上面建的索引表，根据你最终要用的查询条件剪裁一下。*/</span></span><br><span class="line"><span class="comment">/*如果你想跟RDBMS一样建完索引就用，那是不行的，会直接报错，这也是其麻烦的地方*/</span></span><br><span class="line"><span class="operator"><span class="keyword">create</span> <span class="keyword">table</span> my_index</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> _bucketname, <span class="string">`_offsets`</span></span><br><span class="line"><span class="keyword">from</span> default__index_test_table_idx__ <span class="keyword">where</span> <span class="keyword">id</span> = <span class="number">10</span>;</span></span><br><span class="line">	</span><br><span class="line"><span class="comment">/*现在可以用索引了，注意最终查询条件跟上面的剪裁条件一致*/</span></span><br><span class="line"><span class="operator"><span class="keyword">set</span> hive.<span class="keyword">index</span>.<span class="keyword">compact</span>.<span class="keyword">file</span> = /<span class="keyword">user</span>/hive/warehouse/my_index;</span> </span><br><span class="line"><span class="operator"><span class="keyword">set</span> hive.<span class="keyword">input</span>.<span class="keyword">format</span> = org.apache.hadoop.hive.ql.<span class="keyword">index</span>.<span class="keyword">compact</span>.HiveCompactIndexInputFormat;</span></span><br><span class="line"><span class="operator"><span class="keyword">select</span> <span class="keyword">count</span>(*) <span class="keyword">from</span> index_test_table <span class="keyword">where</span> <span class="keyword">id</span> = <span class="number">10</span>;</span></span><br></pre></td></tr></table></figure>
<h2 id="Join算法">Join算法</h2><p>处理分布式join，一般有两种方法:</p>
<blockquote>
<ul>
<li>replication join：把其中一个表复制到所有节点，这样另一个表在每个节点上面的分片就可以跟这个完整的表join了；  </li>
<li>repartition join：把两份数据按照join key进行hash重分布，让每个节点处理hash值相同的join key数据，也就是做局部的join。  </li>
</ul>
</blockquote>
<p>这两种方式在M/R Job中分别对应了map side join和reduce side join。在一些MPP DB中，数据可以按照某列字段预先进行hash分布，这样在跟这个表以这个字段为join key进行join的时候，该表肯定不需要做数据重分布了，这种功能是以HDFS作为底层文件系统的hive所没有的。</p>
<p>在默认情况下，hive的join策略是进行reduce side join。当两个表中有一个是小表的时候，就可以考虑用map join了，因为小表复制的代价会好过大表shuffle的代价。使用map join的配置方法有两种，一种直接在sql中写hint，语法是<code>/*+MAPJOIN (tbl)*/</code>，其中tbl就是你想要做replication的表。另一种方法是设置<code>hive.auto.convert.join = true</code>，这样hive会自动判断当前的join操作是否合适做map join，主要是找join的两个表中有没有小表。至于多大的表算小表，则是由<code>hive.smalltable.filesize</code>决定，默认25MB。</p>
<p>但是有的时候，没有一个表足够小到能够放进内存，但是还是想用map join怎么办？这个时候就要用到bucket map join。其方法是两个join表在join key上都做hash bucket，并且把你打算复制的那个（相对）小表的bucket数设置为大表的倍数。这样数据就会按照join key做hash bucket。小表依然复制到所有节点，map join的时候，小表的每一组bucket加载成hashtable，与对应的一个大表bucket做局部join，这样每次只需要加载部分hashtable就可以了。<br>然后在两个表的join key都具有唯一性的时候（也就是可做主键），还可以进一步做sort merge bucket map join。做法还是两边要做hash bucket，而且每个bucket内部要进行排序。这样一来当两边bucket要做局部join的时候，只需要用类似merge sort算法中的merge操作一样把两个bucket顺序遍历一遍即可完成，这样甚至都不用把一个bucket完整的加载成hashtable，这对性能的提升会有很大帮助。<br>然后这里以一个完整的实验说明这几种join算法如何操作。<br>首先建表要带上bucket：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">create</span> <span class="keyword">table</span> map_join_test(<span class="keyword">id</span> <span class="built_in">int</span>)</span><br><span class="line">clustered <span class="keyword">by</span> (<span class="keyword">id</span>) sorted <span class="keyword">by</span> (<span class="keyword">id</span>) <span class="keyword">into</span> <span class="number">32</span> buckets</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> textfile;</span></span><br></pre></td></tr></table></figure>
<p>然后插入我们准备好的800万行数据，注意要强制划分成bucket（也就是用reduce划分hash值相同的数据到相同的文件）：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">set</span> hive.enforce.bucketing = <span class="literal">true</span>;</span></span><br><span class="line"><span class="operator"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> map_join_test</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> map_join_source_data;</span></span><br></pre></td></tr></table></figure>
<p>这样这个表就有了800万id值（且里面没有重复值，所以可以做sort merge），占用80MB左右。<br>接下来我们就可以一一尝试map join的算法了。首先是普通的map join：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">select</span> <span class="comment">/*+mapjoin(a) */</span><span class="keyword">count</span>(*)</span><br><span class="line"><span class="keyword">from</span> map_join_test a</span><br><span class="line"><span class="keyword">join</span> map_join_test b <span class="keyword">on</span> a.<span class="keyword">id</span> = b.<span class="keyword">id</span>;</span></span><br></pre></td></tr></table></figure>
<p>然后就会看到分发hash table的过程：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="number">2013</span>-<span class="number">08</span>-<span class="number">31</span> <span class="number">09</span>:<span class="number">08</span>:<span class="number">43</span>     Starting to launch local task to process <span class="built_in">map</span> join;      maximum memory = <span class="number">1004929024</span></span><br><span class="line"><span class="number">2013</span>-<span class="number">08</span>-<span class="number">31</span> <span class="number">09</span>:<span class="number">08</span>:<span class="number">45</span>     Processing rows:   <span class="number">200000</span>  Hashtable size: <span class="number">199999</span>  Memory usage:   <span class="number">38823016</span>        rate:   <span class="number">0.039</span></span><br><span class="line"><span class="number">2013</span>-<span class="number">08</span>-<span class="number">31</span> <span class="number">09</span>:<span class="number">08</span>:<span class="number">46</span>     Processing rows:   <span class="number">300000</span>  Hashtable size: <span class="number">299999</span>  Memory usage:   <span class="number">56166968</span>        rate:   <span class="number">0.056</span></span><br><span class="line">……</span><br><span class="line"><span class="number">2013</span>-<span class="number">08</span>-<span class="number">31</span> <span class="number">09</span>:<span class="number">12</span>:<span class="number">39</span>     Processing rows:  <span class="number">4900000</span> Hashtable size: <span class="number">4899999</span> Memory usage:   <span class="number">896968104</span>       rate:   <span class="number">0.893</span></span><br><span class="line"><span class="number">2013</span>-<span class="number">08</span>-<span class="number">31</span> <span class="number">09</span>:<span class="number">12</span>:<span class="number">47</span>     Processing rows:  <span class="number">5000000</span> Hashtable size: <span class="number">4999999</span> Memory usage:   <span class="number">922733048</span>       rate:   <span class="number">0.918</span></span><br><span class="line">Execution failed with <span class="built_in">exit</span> status: <span class="number">2</span></span><br><span class="line">Obtaining error information</span><br><span class="line"></span><br><span class="line">Task failed!</span><br><span class="line">Task ID:</span><br><span class="line">  Stage-<span class="number">4</span></span><br></pre></td></tr></table></figure>
<p>不幸的是，居然内存不够了，直接做map join失败了。但是80MB的大小为何用1G的heap size都放不下？观察整个过程就会发现，平均一条记录需要用到200字节的存储空间，这个overhead太大了，对于map join的小表size一定要好好评估，如果有几十万记录数就要小心了。虽然不太清楚其中的构造原理，但是在互联网上也能找到其他的例证，比如<a href="http://blog.csdn.net/jmydream/article/details/7942529" target="_blank" rel="external">这里</a>和<a href="http://dennyglee.com/2013/04/26/optimizing-joins-running-on-hdinsight-hive-on-azure-at-gfs/" target="_blank" rel="external">这里</a>,平均一行500字节左右。这个明显比一般的表一行占用的数据量要大。不过hive也在做这方面的改进，争取缩小hash table，比如<a href="https://issues.apache.org/jira/browse/HIVE-6430" target="_blank" rel="external">HIVE-6430</a>。</p>
<p>所以接下来我们就用bucket map join，之前分的bucket就派上用处了。只需要在上述sql的前面加上如下的设置：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">set</span> hive.<span class="keyword">optimize</span>.bucketmapjoin = <span class="literal">true</span>;</span></span><br></pre></td></tr></table></figure>
<p>然后还是会看到hash table分发：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="number">2013</span>-<span class="number">08</span>-<span class="number">31</span> <span class="number">09</span>:<span class="number">20</span>:<span class="number">39</span>     Starting to launch local task to process <span class="built_in">map</span> join;      maximum memory = <span class="number">1004929024</span></span><br><span class="line"><span class="number">2013</span>-<span class="number">08</span>-<span class="number">31</span> <span class="number">09</span>:<span class="number">20</span>:<span class="number">41</span>     Processing rows:   <span class="number">200000</span>  Hashtable size: <span class="number">199999</span>  Memory usage:   <span class="number">38844832</span>        rate:   <span class="number">0.039</span></span><br><span class="line"><span class="number">2013</span>-<span class="number">08</span>-<span class="number">31</span> <span class="number">09</span>:<span class="number">20</span>:<span class="number">42</span>     Processing rows:   <span class="number">275567</span>  Hashtable size: <span class="number">275567</span>  Memory usage:   <span class="number">51873632</span>        rate:   <span class="number">0.052</span></span><br><span class="line"><span class="number">2013</span>-<span class="number">08</span>-<span class="number">31</span> <span class="number">09</span>:<span class="number">20</span>:<span class="number">42</span>     Dump the hashtable into file: file:/tmp/hadoop/hive_2013-<span class="number">08</span>-<span class="number">31</span>_21-<span class="number">20</span>-<span class="number">37</span>_444_1135806892100127714/-local-<span class="number">10003</span>/HashTable-Stage-<span class="number">1</span>/MapJoin-a-<span class="number">10</span>-<span class="number">000000</span>_0.hashtable</span><br><span class="line"><span class="number">2013</span>-<span class="number">08</span>-<span class="number">31</span> <span class="number">09</span>:<span class="number">20</span>:<span class="number">46</span>     Upload <span class="number">1</span> File to: file:/tmp/hadoop/hive_2013-<span class="number">08</span>-<span class="number">31</span>_21-<span class="number">20</span>-<span class="number">37</span>_444_1135806892100127714/-local-<span class="number">10003</span>/HashTable-Stage-<span class="number">1</span>/MapJoin-a-<span class="number">10</span>-<span class="number">000000</span>_0.hashtable File size: <span class="number">11022975</span></span><br><span class="line"><span class="number">2013</span>-<span class="number">08</span>-<span class="number">31</span> <span class="number">09</span>:<span class="number">20</span>:<span class="number">47</span>     Processing rows:   <span class="number">300000</span>  Hashtable size: <span class="number">24432</span>   Memory usage:   <span class="number">8470976</span> rate:   <span class="number">0.008</span></span><br><span class="line"><span class="number">2013</span>-<span class="number">08</span>-<span class="number">31</span> <span class="number">09</span>:<span class="number">20</span>:<span class="number">47</span>     Processing rows:   <span class="number">400000</span>  Hashtable size: <span class="number">124432</span>  Memory usage:   <span class="number">25368080</span>        rate:   <span class="number">0.025</span></span><br><span class="line"><span class="number">2013</span>-<span class="number">08</span>-<span class="number">31</span> <span class="number">09</span>:<span class="number">20</span>:<span class="number">48</span>     Processing rows:   <span class="number">500000</span>  Hashtable size: <span class="number">224432</span>  Memory usage:   <span class="number">42968080</span>        rate:   <span class="number">0.043</span></span><br><span class="line"><span class="number">2013</span>-<span class="number">08</span>-<span class="number">31</span> <span class="number">09</span>:<span class="number">20</span>:<span class="number">49</span>     Processing rows:   <span class="number">551527</span>  Hashtable size: <span class="number">275960</span>  Memory usage:   <span class="number">52022488</span>        rate:   <span class="number">0.052</span></span><br><span class="line"><span class="number">2013</span>-<span class="number">08</span>-<span class="number">31</span> <span class="number">09</span>:<span class="number">20</span>:<span class="number">49</span>     Dump the hashtable into file: file:/tmp/hadoop/hive_2013-<span class="number">08</span>-<span class="number">31</span>_21-<span class="number">20</span>-<span class="number">37</span>_444_1135806892100127714/-local-<span class="number">10003</span>/HashTable-Stage-<span class="number">1</span>/MapJoin-a-<span class="number">10</span>-<span class="number">000001</span>_0.hashtable</span><br><span class="line">……</span><br></pre></td></tr></table></figure>
<p>这次就会看到每次构建完一个hash table（也就是所对应的对应一个bucket），会把这个hash table写入文件，重新构建新的hash table。这样一来由于每个hash table的量比较小，也就不会有内存不足的问题，整个sql也能成功运行。不过光光是这个复制动作就要花去3分半的时间，所以如果整个job本来就花不了多少时间的，那这个时间就不可小视。</p>
<p>最后我们试试sort merge bucket map join，在bucket map join的基础上加上下面的设置即可：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">set</span> hive.<span class="keyword">optimize</span>.bucketmapjoin.sortedmerge = <span class="literal">true</span>;</span></span><br><span class="line"><span class="operator"><span class="keyword">set</span> hive.<span class="keyword">input</span>.<span class="keyword">format</span> = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;</span></span><br></pre></td></tr></table></figure>
<p>sort merge bucket map join是不会产生hash table复制的步骤的，直接开始做实际map端join操作了，数据在join的时候边做边读。跳过复制的步骤，外加join算法的改进，使得sort merge bucket map join的效率要明显好于bucket map join。</p>
<p>关于join的算法虽然有这么些选择，但是个人觉得，对于日常使用，掌握默认的reduce join和普通的（无bucket）map join已经能解决大多数问题。如果小表不能完全放内存，但是小表相对大表的size量级差别也非常大的时候，或者是必须要做cross join，那也可以试试bucket map join，不过其hash table分发的过程会浪费不少时间，需要评估下是否能够比reduce join更高效。而sort merge bucket map join虽然性能不错，但是把数据做成bucket本身也需要时间，另外其发动条件比较特殊，就是两边join key必须都唯一（很多介绍资料中都不提这一点。强调下必须都是唯一，哪怕只有一个表不唯一，出来的结果也是错的。当然，其实这点完全可以根据其算法原理推敲出来）。这样的场景相对比较少见，“用户基本表 join 用户扩展表”以及“用户今天的数据快照 join 用户昨天的数据快照”这类场景可能比较合适。</p>
<p>这里顺便说个题外话，在数据仓库中，小表往往是维度表，而小表map join这件事情其实用udf代替还会更快，因为不用单独启动一轮job，所以这也是一种可选方案。当然前提条件是维度表是固定的自然属性（比如日期），只增加不修改（比如网站的页面编号）的情况也可以考虑。如果维度有更新，要做缓慢变化维的，当然还是维表好维护。至于维表原本的一个主要用途OLAP，以Hive目前的性能是没法实现的，也就不需要多虑了。</p>
<h2 id="数据倾斜">数据倾斜</h2><p>所谓数据倾斜，说的是由于数据分布不均匀，个别值集中占据大部分数据量，加上hadoop的计算模式，导致计算资源不均匀引起性能下降。下图就是一个例子：</p>
<p><img src="http://7xltg5.com1.z0.glb.clouddn.com/mr_skew.jpg" alt="mr_skew"></p>
<p>还是拿网站的访问日志说事吧。假设网站访问日志中会记录用户的<code>user_id</code>，并且对于注册用户使用其用户表的<code>user_id</code>，对于非注册用户使用一个<code>user_id = 0</code>代表。那么鉴于大多数用户是非注册用户（只看不写），所以<code>user_id = 0</code>占据了绝大多数。而如果进行计算的时候如果以<code>user_id</code>作为group by的维度或者是join key，那么个别reduce会收到比其他reduce多得多的数据——因为它要接收所有<code>user_id = 0</code>的记录进行处理，使得其处理效果会非常差，其他reduce都跑完很久了它还在运行。</p>
<p>倾斜分成group by造成的倾斜和join造成的倾斜，需要分开看。</p>
<p>group by造成的倾斜有两个参数可以解决，一个是<code>hive.map.aggr</code>，默认值已经为true，他的意思是做map aggregation，也就是在mapper里面做聚合。这个方法不同于直接写mapreduce的时候可以实现的combiner，事实上各种基于mr的框架如pig，cascading等等用的都是map aggregation（或者叫partial aggregation）而非combiner的策略，也就是在mapper里面直接做聚合操作而不是输出到buffer给combiner做聚合。对于map aggregation，hive还会做检查，如果aggregation的效果不好，那么hive会自动放弃map aggregation。判断效果的依据就是经过一小批数据的处理之后，检查聚合后的数据量是否减小到一定的比例，默认是0.5，由<code>hive.map.aggr.hash.min.reduction</code>这个参数控制。所以如果确认数据里面确实有个别取值倾斜，但是大部分值是比较稀疏的，这个时候可以把比例强制设为1，避免极端情况下map aggr失效。<code>hive.map.aggr</code>还有一些相关参数，比如map aggr的内存占用，具体可以参考<a href="http://dev.bizo.com/2013/02/map-side-aggregations-in-apache-hive.html" target="_blank" rel="external">这篇文章</a>。另一个参数是<code>hive.groupby.skewindata</code>。这个参数的意思是做reduce操作的时候，拿到的key并不是所有相同值给同一个reduce，而是随机分发，然后reduce做聚合，做完之后再做一轮MR，拿前面聚合过的数据再算结果。所以这个参数其实跟<code>hive.map.aggr</code>做的是类似的事情，只是拿到reduce端来做，而且要额外启动一轮job，所以其实不怎么推荐用，效果不明显。</p>
<p>如果碰到count distinct的情况需要优化，改写SQL是一个比较简便的方法，可以按照下面这么做：</p>
<figure class="highlight nimrod"><table><tr><td class="code"><pre><span class="line">/*改写前*/</span><br><span class="line">select a, count(<span class="keyword">distinct</span> b) <span class="keyword">as</span> c <span class="keyword">from</span> tbl group by a;</span><br><span class="line">/*改写后*/</span><br><span class="line">select a, count(*) <span class="keyword">as</span> c</span><br><span class="line"><span class="keyword">from</span> (select <span class="keyword">distinct</span> a, b <span class="keyword">from</span> tbl) group by a;</span><br></pre></td></tr></table></figure>
<p>join造成的倾斜，就比如上面描述的网站访问日志和用户表两个表join：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">select</span> a.* <span class="keyword">from</span> <span class="keyword">logs</span> a <span class="keyword">join</span> <span class="keyword">users</span> b <span class="keyword">on</span> a.user_id = b.user_id;</span></span><br></pre></td></tr></table></figure>
<p>hive给出的解决方案叫skew join，其原理把这种<code>user_id = 0</code>的特殊值先不在reduce端计算掉，而是先写入hdfs，然后启动一轮map join专门做这个特殊值的计算，期望能提高计算这部分值的处理速度。当然你要告诉hive这个join是个skew join，即：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">set</span> hive.<span class="keyword">optimize</span>.skewjoin = <span class="literal">true</span>;</span></span><br></pre></td></tr></table></figure>
<p>还有要告诉hive如何判断特殊值，根据<code>hive.skewjoin.key</code>设置的数量hive可以知道，比如默认值是100000，那么超过100000条记录的值就是特殊值。<br>skew join的流程可以用下图描述：</p>
<p><img src="http://7xltg5.com1.z0.glb.clouddn.com/skew_join.jpg" alt="skew_join"></p>
<p>另外对于特殊值的处理往往跟业务有关系，所以也可以从业务角度重写sql解决。比如前面这种倾斜join，可以把特殊值隔离开来（从业务角度说，users表应该不存在<code>user_id = 0</code>的情况，但是这里还是假设有这个值，使得这个写法更加具有通用性）：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">select</span> a.* <span class="keyword">from</span> </span><br><span class="line">(</span><br><span class="line"><span class="keyword">select</span> a.*</span><br><span class="line"><span class="keyword">from</span> (<span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">logs</span> <span class="keyword">where</span> user_id = <span class="number">0</span>)  a </span><br><span class="line"><span class="keyword">join</span> (<span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">users</span> <span class="keyword">where</span> user_id = <span class="number">0</span>) b </span><br><span class="line"><span class="keyword">on</span> a.user_id =  b.user_id</span><br><span class="line"><span class="keyword">union</span> all</span><br><span class="line"><span class="keyword">select</span> a.* </span><br><span class="line"><span class="keyword">from</span> <span class="keyword">logs</span> a <span class="keyword">join</span> <span class="keyword">users</span> b</span><br><span class="line"><span class="keyword">on</span> a.user_id &lt;&gt; <span class="number">0</span> <span class="keyword">and</span> a.user_id = b.user_id</span><br><span class="line">)<span class="keyword">t</span>;</span></span><br></pre></td></tr></table></figure>
<p>大部分时候倾斜是因为某一个特殊值，但是也有极端的情况是因为<strong>某一类特殊值</strong>，这往往是业务设计造成。比如对于商品<code>item_id</code>的编码，除了本身的id序列，还人为的把item的类型也作为编码放在最后两位，这样如果类型1的编码是00，类型2的编码是01，并且类型1是主要商品类，将会造成以00为结尾的商品整体倾斜。这时，如果reduce的数量恰好是100的整数倍，会造成partitioner把00结尾的<code>item_id</code>都hash到同一个reducer，引爆问题。当然，这种情况解决不难，只需要设置合适的reduce值，但是这种坑就会比较隐蔽。</p>
<h1 id="SQL整体优化">SQL整体优化</h1><p>前面对于单个job如何做优化已经做过详细讨论，但是hive查询会生成多个job，针对多个job，有什么地方需要优化？</p>
<h2 id="Job间并行">Job间并行</h2><p>首先，在hive生成的多个job中，在有些情况下job之间是可以并行的，典型的就是子查询。当需要执行多个子查询union all或者join操作的时候，job间并行就可以使用了。比如下面的代码就是一个可以并行的场景示意：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">select</span> * <span class="keyword">from</span> </span><br><span class="line">(</span><br><span class="line">   <span class="keyword">select</span> <span class="keyword">count</span>(*) <span class="keyword">from</span> <span class="keyword">logs</span> </span><br><span class="line">   <span class="keyword">where</span> log_date = <span class="number">20130801</span> <span class="keyword">and</span> item_id = <span class="number">1</span></span><br><span class="line">   <span class="keyword">union</span> all </span><br><span class="line">   <span class="keyword">select</span> <span class="keyword">count</span>(*) <span class="keyword">from</span> <span class="keyword">logs</span> </span><br><span class="line">   <span class="keyword">where</span> log_date = <span class="number">20130802</span> <span class="keyword">and</span> item_id = <span class="number">2</span></span><br><span class="line">   <span class="keyword">union</span> all </span><br><span class="line">   <span class="keyword">select</span> <span class="keyword">count</span>(*) <span class="keyword">from</span> <span class="keyword">logs</span> </span><br><span class="line">   <span class="keyword">where</span> log_date = <span class="number">20130803</span> <span class="keyword">and</span> item_id = <span class="number">3</span></span><br><span class="line">)<span class="keyword">t</span></span></span><br></pre></td></tr></table></figure>
<p>设置job间并行的参数是<code>hive.exec.parallel</code>，将其设为true即可。默认的并行度为8，也就是最多允许sql中8个job并行。如果想要更高的并行度，可以通过<code>hive.exec.parallel. thread.number</code>参数进行设置，但要避免设置过大而占用过多资源。</p>
<h2 id="减少Job数">减少Job数</h2><p>另外在实际开发过程中也发现，一些实现思路会导致生成多余的job而显得不够高效。比如这个需求：查询某网站日志中同时访问过页面a和页面b的用户数量。低效的思路是面向明细的，先取出看过页面a的用户，再取出看过页面b的用户，然后取交集，代码如下：</p>
<figure class="highlight nimrod"><table><tr><td class="code"><pre><span class="line">select count(*) </span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">(select <span class="keyword">distinct</span> user_id </span><br><span class="line"><span class="keyword">from</span> logs where page_name = 'a') a</span><br><span class="line">join </span><br><span class="line">(select <span class="keyword">distinct</span> user_id </span><br><span class="line"><span class="keyword">from</span> logs where blog_owner = 'b') b </span><br><span class="line">on a.user_id = b.user_id;</span><br></pre></td></tr></table></figure>
<p>这样一来，就要产生2个求子查询的job，一个用于关联的job，还有一个计数的job，一共有4个job。<br>但是我们直接用面向统计的方法去计算的话（也就是用group by替代join），则会更加符合M/R的模式，只需要用两个job就能跑完：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">select</span> <span class="keyword">count</span> (*) <span class="keyword">from</span> (</span><br><span class="line"><span class="keyword">select</span> user_id </span><br><span class="line"><span class="keyword">from</span> <span class="keyword">logs</span> <span class="keyword">group</span> <span class="keyword">by</span> user_id</span><br><span class="line"><span class="keyword">having</span> (<span class="keyword">count</span>(<span class="keyword">case</span> <span class="keyword">when</span> page_name = <span class="string">'a'</span> <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">end</span>) *</span><br><span class="line">        <span class="keyword">count</span>(<span class="keyword">case</span> <span class="keyword">when</span> page_name = <span class="string">'b'</span> <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">end</span>) &gt; <span class="number">0</span>)</span><br><span class="line">)<span class="keyword">t</span>;</span></span><br></pre></td></tr></table></figure>
<p>第一种查询方法符合思考问题的直觉，是工程师和分析师在实际查数据中最先想到的写法，但是如果在目前hive的query planner不是那么智能的情况下，想要更加快速的跑出结果，懂一点工具的内部机理也是必须的。</p>
<p><strong>2015.01 updated:</strong> 最近本文被CSDN<a href="http://www.csdn.net/article/2015-01-13/2823530" target="_blank" rel="external">转载</a>。时隔一年多，hive已经有了很多变化，当然本文中的方法都还是适用的。本文中的一些内容（比如存储格式）已经有了更好的解决办法，在我比较新的blog中也有间接的体现。但是碍于精力有限，不会专门在本文中更新相关内容了。另外有网友指出原来文章中最后一段代码是有问题的，经检查确实是我的疏忽，描述也略有问题，现已在本文中改正。当然原有代码体现出来的思路是没有问题的，主要是语法细节的错误。</p>
<p><strong>2015.12 updated:</strong> 更新了关于<code>hive.map.aggr</code>的解释，并且补充了因为对字段人为编码而造成的数据倾斜的案例。</p>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2013/04/30/数据仓库中的sql性能优化（mysql篇）/" itemprop="url">
                数据仓库中的sql性能优化（MySQL篇）
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2013-04-30T17:46:17+08:00" content="2013-04-30">
            2013-04-30
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/data-system/" itemprop="url" rel="index">
                  <span itemprop="name">data system</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2013/04/30/数据仓库中的sql性能优化（mysql篇）/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2013/04/30/数据仓库中的sql性能优化（mysql篇）/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><p>做数据仓库的头两年，使用高配置单机 + MySQL的方式来实现所有的计算（包括数据的ETL，以及报表计算。没有OLAP）。用过MySQL自带的MYISAM和列存储引擎Infobright。这篇文章总结了自己和团队在那段时间碰到的一些常见性能问题和解决方案。</p>
<p>P.S.如果没有特别指出，下面说的mysql都是指用MYISAM做存储引擎。</p>
<h1 id="利用已有数据，避免重复计算">利用已有数据，避免重复计算</h1><p>业务需求中往往有计算一周/一个月的某某数据，比如计算最近一周某个特定页面的PV/UV。这里出现的问题就是实现的时候直接取整周的日志数据，然后进行计算。这样其实就出现了重复计算，某一天的数据在不同的日子里被重复计算了7次。</p>
<p>解决办法非常之简单，就是把计算进行切分，如果是算PV，做法就是每天算好当天的PV，那么一周的PV就把算好的7天的PV相加。如果是算UV，那么每天从日志数据取出相应的访客数据，把最近七天的访客数据单独保存在一个表里面，计算周UV的时候直接用这个表做计算，而不需要从原始日志数据中抓上一大把数据来算了。</p>
<p>这是一个非常简单的问题，甚至不需要多少SQL的知识，但是在开发过程中往往被视而不见。这就是只实现业务而忽略性能的表现。从小规模数据仓库做起的工程师，如果缺乏这方面的意识和做事规范，就容易出现这种问题，等到数据仓库的数据量变得比较大的时候，才会发现。</p>
<h1 id="case_when关键字的使用方法">case when关键字的使用方法</h1><p><code>case when</code>这个关键字，在做聚合的时候，可以很方便的将一份数据在一个SQL语句中进行分类的统计。举个例子，比如下面有一张成绩表(表名定为<code>scores</code>)： </p>
<table>
<thead>
<tr>
<th>name</th>
<th>course</th>
<th>score</th>
</tr>
</thead>
<tbody>
<tr>
<td>小明</td>
<td>语文</td>
<td>90</td>
</tr>
<tr>
<td>小张</td>
<td>语文</td>
<td>94</td>
</tr>
<tr>
<td>小红</td>
<td>语文</td>
<td>95</td>
</tr>
<tr>
<td>小明</td>
<td>数学</td>
<td>96</td>
</tr>
<tr>
<td>小张</td>
<td>数学</td>
<td>98</td>
</tr>
<tr>
<td>小红</td>
<td>数学</td>
<td>94</td>
</tr>
<tr>
<td>小明</td>
<td>英语</td>
<td>99</td>
</tr>
<tr>
<td>小张</td>
<td>英语</td>
<td>96</td>
</tr>
<tr>
<td>小红</td>
<td>英语</td>
<td>93</td>
</tr>
</tbody>
</table>
<p>现在需要统计小张的平均成绩，小明的平均成绩和小明的语文成绩。SQL实现如下：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">select</span> </span><br><span class="line">        <span class="keyword">avg</span> (<span class="keyword">case</span> <span class="keyword">when</span> <span class="keyword">name</span> =<span class="string">'小张'</span> <span class="keyword">then</span> score <span class="keyword">end</span>) <span class="keyword">as</span> xz_avg_score,</span><br><span class="line">        <span class="keyword">avg</span> (<span class="keyword">case</span> <span class="keyword">when</span> <span class="keyword">name</span> =<span class="string">'小明'</span> <span class="keyword">then</span> score <span class="keyword">end</span>) <span class="keyword">as</span> xm_avg_score,</span><br><span class="line">        <span class="keyword">avg</span> (<span class="keyword">case</span> <span class="keyword">when</span> <span class="keyword">name</span> =<span class="string">'小明'</span> <span class="keyword">and</span> course = <span class="string">'语文'</span> <span class="keyword">then</span> score <span class="keyword">end</span>) <span class="keyword">as</span> xm_yuwen_score </span><br><span class="line"><span class="keyword">from</span> scores;</span></span><br></pre></td></tr></table></figure>
<p>如果现在这个成绩表有1200万条的数据，包含了400万的名字 * 3个科目，上面的计算需要多长时间？我做了一个简单的测试，答案是5.5秒。<br>而如果我们把sql改成下面的写法：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">select</span> </span><br><span class="line">       <span class="keyword">avg</span> (<span class="keyword">case</span> <span class="keyword">when</span> <span class="keyword">name</span> =<span class="string">'小张'</span> <span class="keyword">then</span> score <span class="keyword">end</span>) <span class="keyword">as</span> xz_avg_score,</span><br><span class="line">       <span class="keyword">avg</span> (<span class="keyword">case</span> <span class="keyword">when</span> <span class="keyword">name</span> =<span class="string">'小明'</span> <span class="keyword">then</span> score <span class="keyword">end</span>) <span class="keyword">as</span> xm_avg_score,</span><br><span class="line">       <span class="keyword">avg</span> (<span class="keyword">case</span> <span class="keyword">when</span> <span class="keyword">name</span> =<span class="string">'小明'</span> <span class="keyword">and</span> course = <span class="string">'语文'</span> <span class="keyword">then</span> score <span class="keyword">end</span>) </span><br><span class="line"><span class="keyword">as</span> xm_yuwen_score </span><br><span class="line"><span class="keyword">from</span> scores <span class="keyword">where</span> <span class="keyword">name</span> <span class="keyword">in</span> (<span class="string">'小张'</span>, <span class="string">'小明'</span>);</span></span><br></pre></td></tr></table></figure>
<p>这样的话，只需要3.3秒就能完成。</p>
<p>之所以后面一种写法总是比前面一种写法快，不同之处就在于是否先在<code>where</code>里面把数据过滤掉。前一种写法扫描了三遍全表的数据（做一个<code>case when</code>扫一遍），后面的写法扫描一遍全表，把数据过滤了之后，<code>case when</code>就不用过这么多数据量了。</p>
<p>跟进一步说，如果在name字段上有索引，那么后一种写法将会更快，测试结果只用0.05秒，而前面一种情况，sql优化器是判断不出来能用索引的，时间依然是5.5秒。</p>
<p>在实际工作中，开发经常只是为了实现功能逻辑，而习惯了在<code>case when</code>中限制条件取数据。这样在出现类似例子中的需求时，没有把应该限制的条件写到<code>where</code>里面。这是在实际代码中发现最多的一类问题。</p>
<h1 id="分页取数方式">分页取数方式</h1><p>在数据仓库中有一个重要的基础步骤，就是对数据进行清洗。比如数据源的数据如果以JSON方式存储，在mysql的数据仓库就必须将json中需要的字段提取出来，做成单独的表字段。这个步骤用sql直接处理很麻烦，所以可以用主流编程语言（比如java）的json库进行解析。解析的时候需要读取数据，一次性读取进来是不可能的，所以要分批读取（相当于分页了）。</p>
<p>最初的实现方式就是标记住每次取数据的偏移量，然后一批批读取：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">select</span> json_obj <span class="keyword">from</span> <span class="keyword">t</span> <span class="keyword">limit</span> <span class="number">10000</span>,<span class="number">10000</span>;</span></span><br><span class="line"><span class="operator"><span class="keyword">select</span> json_obj <span class="keyword">from</span> <span class="keyword">t</span> <span class="keyword">limit</span> <span class="number">20000</span>,<span class="number">10000</span>;</span></span><br><span class="line"><span class="operator"><span class="keyword">select</span> json_obj <span class="keyword">from</span> <span class="keyword">t</span> <span class="keyword">limit</span> <span class="number">30000</span>,<span class="number">10000</span>;</span></span><br><span class="line"><span class="comment">/*略去很多行……*/</span></span><br><span class="line"><span class="operator"><span class="keyword">select</span> json_obj <span class="keyword">from</span> <span class="keyword">t</span> <span class="keyword">limit</span> <span class="number">990000</span>,<span class="number">10000</span>;</span></span><br></pre></td></tr></table></figure>
<p>这样的代码，在开始几句sql的时候执行速度还行，但是到后面会越来越慢，因为每次要读取大量数据再丢弃，其实是一种浪费。</p>
<p>高效的实现方式，可以是用表中的主键进行分页。如果数据是按照主键排序的，那么可以是这样（这么做是要求主键的取值序列是连续的。假设主键的取值序列我们比较清楚，是从10001-1000000的连续值）：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">select</span> json_obj <span class="keyword">from</span> <span class="keyword">t</span> <span class="keyword">where</span> <span class="keyword">t</span>.<span class="keyword">id</span> &gt; <span class="number">10000</span> <span class="keyword">limit</span> <span class="number">10000</span>;</span></span><br><span class="line"><span class="operator"><span class="keyword">select</span> json_obj <span class="keyword">from</span> <span class="keyword">t</span> <span class="keyword">where</span> <span class="keyword">t</span>.<span class="keyword">id</span> &gt; <span class="number">20000</span> <span class="keyword">limit</span> <span class="number">10000</span>;</span></span><br><span class="line"><span class="operator"><span class="keyword">select</span> json_obj <span class="keyword">from</span> <span class="keyword">t</span> <span class="keyword">where</span> <span class="keyword">t</span>.<span class="keyword">id</span> &gt; <span class="number">30000</span> <span class="keyword">limit</span> <span class="number">10000</span>;</span></span><br><span class="line"><span class="comment">/*略去很多行……*/</span></span><br><span class="line"><span class="operator"><span class="keyword">select</span> json_obj <span class="keyword">from</span> <span class="keyword">t</span> <span class="keyword">where</span> <span class="keyword">t</span>.<span class="keyword">id</span> &gt; <span class="number">990000</span> <span class="keyword">limit</span> <span class="number">10000</span>;</span></span><br></pre></td></tr></table></figure>
<p>就算数据不是按主键排序的，也可以通过限制主键的范围来分页。这样处理的话，主键的取值序列不连续也没有太大问题，就是每次拿到的数据会比理想中的少一些，反正是用在数据处理，不影响正确性：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">select</span> json_obj <span class="keyword">from</span> <span class="keyword">t</span> <span class="keyword">where</span> <span class="keyword">t</span>.<span class="keyword">id</span> &gt; <span class="number">10000</span> <span class="keyword">and</span> <span class="keyword">t</span>.<span class="keyword">id</span> &lt;= <span class="number">20000</span>;</span></span><br><span class="line"><span class="operator"><span class="keyword">select</span> json_obj <span class="keyword">from</span> <span class="keyword">t</span> <span class="keyword">where</span> <span class="keyword">t</span>.<span class="keyword">id</span> &gt; <span class="number">20000</span> <span class="keyword">and</span> <span class="keyword">t</span>.<span class="keyword">id</span> &lt;= <span class="number">30000</span>;</span></span><br><span class="line"><span class="operator"><span class="keyword">select</span> json_obj <span class="keyword">from</span> <span class="keyword">t</span> <span class="keyword">where</span> <span class="keyword">t</span>.<span class="keyword">id</span> &gt; <span class="number">30000</span> <span class="keyword">and</span> <span class="keyword">t</span>.<span class="keyword">id</span> &lt;= <span class="number">40000</span>;</span></span><br><span class="line"><span class="comment">/*略去很多行……*/</span></span><br><span class="line"><span class="operator"><span class="keyword">select</span> json_obj <span class="keyword">from</span> <span class="keyword">t</span> <span class="keyword">where</span> <span class="keyword">t</span>.<span class="keyword">id</span> &gt; <span class="number">990000</span> <span class="keyword">and</span> <span class="keyword">t</span>.<span class="keyword">id</span> &lt;= <span class="number">1000000</span>;</span></span><br></pre></td></tr></table></figure>
<p>这样的话，由于主键上面有索引，取数据速度就不会受到数据的具体位置的影响了。</p>
<h1 id="索引使用">索引使用</h1><p>索引的使用是关系数据库的SQL优化中一个非常重要的主题，也是一个常识性的东西。但是工程师在实际开发中往往是加完索引就觉得万事大吉了，也不去检查索引是否被正确的使用了，所以会经常出一些瞎猫碰到死耗子或者是似是而非的情况。</p>
<h2 id="索引调整">索引调整</h2><p>前面说到开发人员在对索引的了解似是而非的时候只知道要加索引，而不知道为什么加。</p>
<p>比如现在有一个数据集，对应非常常见的网站统计场景。这个数据集有两个表组成，其中一个是流量表<code>item_visits</code>，每条记录表示某一个商品（item）被访问了一次，包括访问者的一些信息，比如用户id，用户名等等，有将近800万条数据。示例如下：</p>
<table>
<thead>
<tr>
<th>item_id</th>
<th>visitor_id</th>
<th>visitor_name</th>
<th>visitor_city</th>
<th>url</th>
<th>……</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>55</td>
<td>用户001</td>
<td>1</td>
<td>……</td>
<td>……</td>
</tr>
<tr>
<td>10</td>
<td>245</td>
<td>用户002</td>
<td>2</td>
<td>……</td>
<td>……</td>
</tr>
<tr>
<td>3</td>
<td>2</td>
<td>用户003</td>
<td>1</td>
<td>……</td>
<td>……</td>
</tr>
<tr>
<td>10</td>
<td>148</td>
<td>用户004</td>
<td>3</td>
<td>……</td>
<td>……</td>
</tr>
<tr>
<td>3</td>
<td>75</td>
<td>用户005</td>
<td>4</td>
<td>……</td>
<td>……</td>
</tr>
<tr>
<td>7</td>
<td>422</td>
<td>用户006</td>
<td>4</td>
<td>……</td>
<td>……</td>
</tr>
<tr>
<td>3</td>
<td>10</td>
<td>用户007</td>
<td>……</td>
<td>……</td>
<td>……</td>
</tr>
<tr>
<td>……</td>
<td>……</td>
<td>……</td>
<td>……</td>
<td>……</td>
<td>……</td>
</tr>
</tbody>
</table>
<p>另一个表是商品表<code>items</code>，包含1200多种商品，字段有商品名字和所属种类：</p>
<table>
<thead>
<tr>
<th>item_id</th>
<th>item_name</th>
<th>item_type</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>毛巾</td>
<td>生活用品</td>
</tr>
<tr>
<td>2</td>
<td>脸盆</td>
<td>生活用品</td>
</tr>
<tr>
<td>……</td>
<td>……</td>
<td>……</td>
</tr>
</tbody>
</table>
<p>现在有一个需求，计算每个商品种类（item_type）被访问的次数。sql的实现不难：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">select</span> item_type, <span class="keyword">count</span>(*) <span class="keyword">as</span> visit_num</span><br><span class="line"><span class="keyword">from</span> items a</span><br><span class="line"><span class="keyword">join</span> item_visit b <span class="keyword">using</span> (item_id)</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> item_type;</span></span><br></pre></td></tr></table></figure>
<p>开发人员知道，在join的时候，其中的一个表的join key要加索引，然后他发现<code>visit</code>表在<code>item_id</code>字段上已经有索引了，所以就打完收工了。到这里为止一切都没有问题。但是后来这个需求有改动，需要限制用户的城市是某个固定城市，比如<code>visitor_city = 1</code>，那么显然sql变成了：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">select</span> item_type, <span class="keyword">count</span>(*) <span class="keyword">as</span> visit_num</span><br><span class="line"><span class="keyword">from</span> items a</span><br><span class="line"><span class="keyword">join</span> item_visit b <span class="keyword">using</span> (item_id)</span><br><span class="line"><span class="keyword">where</span> visitor_city = <span class="number">1</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> item_type;</span></span><br></pre></td></tr></table></figure>
<p>开发人员按照需求修改sql之后对于索引的调整无动于衷，因为他觉得，我已经用上索引了呀。而实际上很明显的，只需要在<code>visitor_city</code>和<code>items</code>表的<code>item_id</code>上都加上索引，就能极大的减少时间。原因就在于开发人员“感觉”能用上索引，而且开发阶段试运行时间没问题就OK，并不关心是不是有更好的索引使用方式，甚至不确认是否用上了索引。在这样的一个真实案例中，原有的sql在后期出现了运行缓慢的现象，才逐渐被发掘出问题。</p>
<h2 id="覆盖索引">覆盖索引</h2><p>针对上面那个需求（不限制city），假设现在两个表的的item_id字段都有索引，而且把<code>count(*)</code>换成<code>count(visitor_city)</code>，还是会得到完全一样的结果，但是会对执行时间有什么影响？</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">select</span> item_type, <span class="keyword">count</span>(visitor_city) <span class="keyword">as</span> visit_num</span><br><span class="line"><span class="keyword">from</span> items a</span><br><span class="line"><span class="keyword">join</span> item_visit b <span class="keyword">using</span> (item_id)</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> item_type;</span></span><br></pre></td></tr></table></figure>
<p>测试结果表明，<code>count(*)</code>版本用时57秒，<code>count(visitor_city)</code>版本用时70秒。原因在哪里？主要就在于<code>count(*)</code>版本中<code>item_visits</code>表只需要用到<code>item_id</code>，所以可以直接用索引来代替数据访问，这就是覆盖索引。<br>在实际开发中，一方面要创造使用覆盖索引的机会，不要无谓的增加不需要的字段到查询语句中，上面的案例就是反面例子，实际开发中就有这样的情况发生。另一方面，根据具体的查询也要在成本允许的情况下构造覆盖索引，这样比普通的索引有更少的IO，自然有更快的访问速度。</p>
<h2 id="强制索引">强制索引</h2><p>有时候mysql的执行计划会不恰当的使用索引，这个时候就要求开发人员有一定的排查能力，并且根据实际情况调整索引。当然，这种情况还是比较少见的。Mysql用错索引的主要原因在于mysql是根据IO和CPU的代价来估算是否用索引，或者用哪种索引，而这个估算基于的统计信息有可能不准确。</p>
<p>强制使用索引主要在两种场景下碰到。第一种是针对where过滤条件的索引，这个时候的语法是<code>force index (index_name)</code>。比如在ETL中常见的数据抽取，利用时间戳增量抽取当天新增或者更新的数据：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">user</span> <span class="keyword">where</span> update_timestamp &gt;= <span class="keyword">curdate</span>() – <span class="number">1</span>;</span></span><br></pre></td></tr></table></figure>
<p>一般在更新时间戳上会有索引，但是有时候mysql会判断出某一天的更新量特别大，比如超过了20%，那么根据数据的选择性，mysql决定不用索引。但实际上这个判断有可能是不准确的，如果表比较大而且在线上服务时间较长，还是有可能发生的，这个时候可以通过强制使用索引保证抽取的稳定性（当然，这要基于你对业务的了解，保证抽取量能维持在一个稳定的水平，不会发生超大更新量的情况）：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">user</span> <span class="keyword">force</span> <span class="keyword">index</span> (update_timestamp) </span><br><span class="line"><span class="keyword">where</span> update_timestamp &gt;= <span class="keyword">curdate</span>() – <span class="number">1</span>;</span></span><br></pre></td></tr></table></figure>
<p>强制使用索引的第二种情况，是join时对索引的选择。数据仓库中有时候会出现一种计算场景，对一个按日统计的报表中某一天的小部分数据进行更新。比如有一个按日统计的用户pv表（<code>user_pv_byday</code>，一天约50万用户，表中有1个月数据，共1500万）：</p>
<table>
<thead>
<tr>
<th>user_id</th>
<th>pv</th>
<th>stat_date</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>10</td>
<td>2013-01-01</td>
</tr>
<tr>
<td>2</td>
<td>15</td>
<td>2013-01-01</td>
</tr>
<tr>
<td>……</td>
<td>……</td>
<td>……</td>
</tr>
<tr>
<td>1</td>
<td>14</td>
<td>2013-01-02</td>
</tr>
<tr>
<td>2</td>
<td>19</td>
<td>2013-01-02</td>
</tr>
<tr>
<td>……</td>
<td>……</td>
<td>……</td>
</tr>
</tbody>
</table>
<p>另一个表是当天小部分用户的pv表（<code>user_pv_to_update</code>，1000条数据）：</p>
<table>
<thead>
<tr>
<th>user_id</th>
<th>pv</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>18</td>
</tr>
<tr>
<td>2</td>
<td>20</td>
</tr>
<tr>
<td>……</td>
<td>……</td>
</tr>
</tbody>
</table>
<p>两个表的索引情况是，<code>user_pv_byday</code>表的<code>user_id</code>和<code>stat_date</code>字段有索引，<code>user_pv_to_update</code>表的<code>user_id</code>字段有索引。<br>现在要把<code>user_pv_to_update</code>的pv数据更新到<code>user_pv_byday</code>当天的数据中：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">update</span> user_pv_byday a </span><br><span class="line"><span class="keyword">join</span> user_pv_to_update b <span class="keyword">on</span> a.user_id = b.user_id </span><br><span class="line"><span class="keyword">set</span> a.pv = b.pv</span><br><span class="line"><span class="keyword">where</span> a.stat_date = <span class="keyword">curdate</span>() – <span class="number">1</span>;</span></span><br></pre></td></tr></table></figure>
<p>经过一段时间的线上运行之后，发现这个步骤越来越慢了。查了一下执行计划，发现mysql选择了<code>user_pv_byday</code>表的<code>user_id</code>做索引。于是，决定强制用上<code>stat_date</code>的索引。这样一来join的时候需要用到的就是<code>user_pv_to_update</code>上的<code>user_id</code>字段。这个时候就需要指定顺序，强制<code>user_pv_byday</code>表作为外层驱动表（<code>user_pv_to_update</code>则是nest loop的内层嵌套）：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">update</span> user_pv_byday a </span><br><span class="line"><span class="keyword">straight_join</span> user_pv_to_update b <span class="keyword">on</span> a.user_id = b.user_id </span><br><span class="line"><span class="keyword">set</span> a.pv = b.pv</span><br><span class="line"><span class="keyword">where</span> a.stat_date = <span class="keyword">curdate</span>() – <span class="number">1</span>;</span></span><br></pre></td></tr></table></figure>
<p>然后来看一下两种写法的执行计划。</p>
<p><strong>原有写法：</strong></p>
<table>
<thead>
<tr>
<th>……</th>
<th>table</th>
<th>type</th>
<th>……</th>
<th>key</th>
<th>……</th>
<th>ref</th>
<th>rows</th>
<th>extra</th>
</tr>
</thead>
<tbody>
<tr>
<td>……</td>
<td>b</td>
<td>ALL</td>
<td>……</td>
<td></td>
<td>……</td>
<td></td>
<td>1000</td>
<td></td>
</tr>
<tr>
<td>……</td>
<td>a</td>
<td>ref</td>
<td>……</td>
<td>user_id</td>
<td>……</td>
<td>b.user_id</td>
<td>368</td>
<td>Using where</td>
</tr>
</tbody>
</table>
<p><strong>强制join顺序的写法：</strong></p>
<table>
<thead>
<tr>
<th>……</th>
<th>table</th>
<th>type</th>
<th>……</th>
<th>key</th>
<th>……</th>
<th>ref</th>
<th>rows</th>
<th>extra</th>
</tr>
</thead>
<tbody>
<tr>
<td>……</td>
<td>a</td>
<td>ref</td>
<td>……</td>
<td>stat_date</td>
<td>……</td>
<td>const</td>
<td>485228</td>
<td>Using where</td>
</tr>
<tr>
<td>……</td>
<td>b</td>
<td>ref</td>
<td>……</td>
<td>user_id</td>
<td>……</td>
<td>a.user_id</td>
<td>1</td>
<td>Using where</td>
</tr>
</tbody>
</table>
<p>根据执行计划中的数据，可以说mysql的选择没有错。原有写法需要读取的数据大致是1000 * 368约合37万，修改写法则是48万，修改写法读取的代价更大。但实际运行情况则是修改写法快过原来写法数倍。</p>
<p>不过，这个情况却不能随时重现。真要把这两个表写入空表并且重建索引再来查询，会发现<code>straight_join</code>的结果确实会更慢，也就是说在初始状态下，mysql的判断是对的。所以这样的问题只在日常运营中才会发生，无法重现，却是真实存在的，而且碰到类似这一类的应用场景，则必然发生。</p>
<p>究其原因，在这种情况下，由于数据不是一次性建成，而是按天陆续写入，所以<code>user_pv_byday</code>的<code>user_id</code>索引会进行反复的修改，造成索引碎片，极端严重的情况下还会导致索引完全失效。而<code>stat_date</code>上的索引由于是每天递增，所以完全没有碎片问题，而且读取数据是还是顺序读取，效率自然要高不少。另外，根据实际情况的观察，随着数据的积累，上面执行计划中368这个值还会变得更小，也就是统计信息会越来越不准确。</p>
<p>总之，mysql的执行计划在大部分情况下是没问题的，但是随着数据的不断积累修改，会逐渐出现mysql所不了解的细节，影响优化器的正常判断。这个时候如果能对表做一下重建也能让事情回到正轨，但是很多时候没有这个权限或者条件去做（比如你不是DBA，没有这种操作权限；或者表太大，没有完整的时间段可以操作）。那么强制索引使用就成了开发人员一个低成本的解决方案。</p>
<h1 id="过多的join">过多的join</h1><p>在mysql中，需要join的表如果太多，会对性能造成很显著的下降。同样，举例说明。</p>
<p>首先生成一个表（表名<code>test</code>），这个表只有60条记录，6个字段，其中第一个字段为主键：</p>
<table>
<thead>
<tr>
<th>pk</th>
<th>c1</th>
<th>c2</th>
<th>c3</th>
<th>c4</th>
<th>c5</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>11</td>
<td>21</td>
<td>31</td>
<td>41</td>
<td>51</td>
</tr>
<tr>
<td>2</td>
<td>12</td>
<td>22</td>
<td>32</td>
<td>42</td>
<td>52</td>
</tr>
<tr>
<td>3</td>
<td>13</td>
<td>23</td>
<td>33</td>
<td>43</td>
<td>53</td>
</tr>
<tr>
<td>4</td>
<td>14</td>
<td>24</td>
<td>34</td>
<td>44</td>
<td>54</td>
</tr>
<tr>
<td>……</td>
<td>……</td>
<td>……</td>
<td>……</td>
<td>……</td>
<td>……</td>
</tr>
</tbody>
</table>
<p>然后做一个查询：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">select</span> <span class="keyword">count</span>(*) </span><br><span class="line"><span class="keyword">from</span> <span class="keyword">test</span> a1</span><br><span class="line"><span class="keyword">join</span> <span class="keyword">test</span> a2 <span class="keyword">using</span> (pk)</span></span><br></pre></td></tr></table></figure>
<p>也就是说让test表跟自己关联。计算的结果显然是60，而且几乎不费时间。</p>
<p>但是如果是这样的查询（十个<code>test</code>表关联），会花费多少时间？</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">select</span> <span class="keyword">count</span>(*)</span><br><span class="line"><span class="keyword">from</span> <span class="keyword">test</span> a1</span><br><span class="line"><span class="keyword">join</span> <span class="keyword">test</span> a2 <span class="keyword">using</span> (pk)</span><br><span class="line"><span class="keyword">join</span> <span class="keyword">test</span> a3 <span class="keyword">using</span> (pk)</span><br><span class="line"><span class="keyword">join</span> <span class="keyword">test</span> a4 <span class="keyword">using</span> (pk)</span><br><span class="line"><span class="keyword">join</span> <span class="keyword">test</span> a5 <span class="keyword">using</span> (pk)</span><br><span class="line"><span class="keyword">join</span> <span class="keyword">test</span> a6 <span class="keyword">using</span> (pk)</span><br><span class="line"><span class="keyword">join</span> <span class="keyword">test</span> a7 <span class="keyword">using</span> (pk)</span><br><span class="line"><span class="keyword">join</span> <span class="keyword">test</span> a8 <span class="keyword">using</span> (pk)</span><br><span class="line"><span class="keyword">join</span> <span class="keyword">test</span> a9 <span class="keyword">using</span> (pk)</span><br><span class="line"><span class="keyword">join</span> <span class="keyword">test</span> a10 <span class="keyword">using</span> (pk)</span></span><br></pre></td></tr></table></figure>
<p>答案是：肯定超过5分钟。因为做了实际测试，5分钟还没有出结果。</p>
<p>那么mysql到底在干什么呢？用show processlist去看一下运行时情况：</p>
<table>
<thead>
<tr>
<th>ID</th>
<th>……</th>
<th>COMMAND</th>
<th>TIME</th>
<th>STATE</th>
<th>INFO</th>
</tr>
</thead>
<tbody>
<tr>
<td>121</td>
<td>……</td>
<td>QUERY</td>
<td>302</td>
<td>statistics</td>
<td>select count(*) from test a1 ……</td>
</tr>
</tbody>
</table>
<p>原来是处在<code>statistics</code>的状态。这个状态，根据mysql的解释是在根据统计信息去生成执行计划。当然这个解释肯定是没有追根溯源。实际上mysql在生成执行计划的时候，其中有一个步骤，是确定表的join顺序。默认情况下，mysql会把所有join顺序全部排列出来，依次计算各个join顺序的执行代价并且取最优的那个。这样一来，n个表join会有n!种情况。十个表join就是10!，大概300万，所以难怪mysql要分析半天了。</p>
<p>而在实际开发过程中，曾经出现过30多个表关联的情况（有10^32种join顺序）。一旦出现，花费在statistics状态的时间往往是在1个小时以上，这还只是在表数据量都非常小，需要做顺序分析的点比较少的情况下。至于出现这种情况的原因，无外乎我们需要计算的汇总报表的字段太多，需要从各种各样的地方计算出来数据，然后再把数据拼接起来，报表在维护过程中不断添加字段，又由于种种原因没有去掉已经废弃的字段，这样字段必定会越来愈多，实现这些字段计算就需要用更多的临时计算结果表去关联到一起，结果需要关联的表也越来越多，成了mysql无法承受之重。</p>
<p>这个问题的解决方法有两个。从开发角度来说，可以控制join的表个数。如果需要join的表太多，可以根据业务上的分类，先做一轮join，把表的数量控制在一定范围内，然后拿到第一轮的join结果，再做第二轮全局join，这样就不会有问题了。从运维角度来说，可以设置<code>optimizer_search_depth</code>这个参数。它能够控制join顺序遍历的深度，进行贪婪搜索得到局部最优的顺序。一般有好多个表join的情况，都是上面说的相同维度的数据需要拼接成一张大表，对于join顺序基本上没什么要求。所以适当的把这个值调低，对于性能应该说没有影响。</p>
<h1 id="列存储引擎Infobright">列存储引擎Infobright</h1><p>Infobright是基于mysql的存储引擎，具有列存储/列压缩和知识网格等特性，比较适合数据仓库的计算。使用起来也不需要考虑索引之类的问题，非常方便。不过经过一段时间的运用，也发现了个别需要注意的问题。</p>
<p>一个问题和myisam类似，不要取不需要的数据。这里说的不需要的数据，包括不需要的列（Infobright的使用常识。当然行存储也要注意，只不过影响相对比较小，所以没有专门提到），和不需要的行（行数是可以扩展的，行存储一行基本上都能存在一个存储单元中，但是列存储一列明显不可能存在一个存储单元中）。</p>
<p>第二个问题，就是Infobright在长字符检索的时候并不给力。一般来说，网站的访问日志中会有URL字段用来标识访问的具体地址。这样就有查找特定URL的需求。比如我要在一个日志表中查找某种类型的url的访问次数：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">select</span> <span class="keyword">count</span>(*) <span class="keyword">from</span> <span class="keyword">log</span> <span class="keyword">where</span> <span class="keyword">url</span> <span class="keyword">like</span> <span class="string">'%mysql%'</span>;</span></span><br></pre></td></tr></table></figure>
<p>类似这样在一个长字符串里面检索子串的需求，Infobright的执行时间测试下来是myisam的1.5-3倍。<br>至于速度慢的原因，这里给出一个简要的解释：Infobright作为列式数据库使用了列存储的常用特性，就是压缩（列式数据库的压缩率一般要能做到10%以内，Infobright也不例外）。另外为了加快查找速度，它还使用了一种叫知识网格检索方式，一般情况下能够极大的减少需要读取的数据量。关于知识网格的原理已经超出了本篇文章的讨论篇幅，可以看<a href="http://www.cnblogs.com/inmanhust/archive/2010/05/08/1730343.html" target="_blank" rel="external">这里</a>了解。但是在查询url的时候，知识网格的优点无法体现出来，但是使用知识网格本身带来的检索代价和解压长字符串的代价却仍然存在，而且比查询一般的数字类字段要来的大的多。</p>
<p>解决办法有几种，比如<a href="http://www.infobright.org/images/uploads/blogs/how-to/How_To_Efficiently_Search_Strings_in_Infobright.pdf" target="_blank" rel="external">官方的方案</a>是把长字符串MD5成一个数字，查询的时候加上数字作为补充查询条件。而<a href="http://weibo.com/1699016425/zzPJbzQFz" target="_blank" rel="external">这条微博</a>给出的方法是进行分词然后再整数化。这些方案相对来说比较复杂，而我尝试过一种简单的解决方案（不过也有相当的局限性），就是根据这个长字段排序后再导入。这样一来按照该字段查询时，通过知识网格就能够屏蔽掉比较多的“数据包”（Infobright的数据压缩单元），而未排序的情况下符合条件的数据散布在各个“数据包”中，其解压工作量就大得多了。使用这个方法进行查询，测试下来其执行时间就只有mysql的0.5倍左右了。 </p>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
 </div>

        

        
      </div>

      
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      <section class="site-overview">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" src="http://7xltg5.com1.z0.glb.clouddn.com/author.jpg" alt="奔跑的兔子" itemprop="image"/>
          <p class="site-author-name" itemprop="name">奔跑的兔子</p>
        </div>
        <p class="site-description motion-element" itemprop="description"></p>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">7</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          <div class="site-state-item site-state-categories">
            <a href="/categories">
              <span class="site-state-item-count">1</span>
              <span class="site-state-item-name">分类</span>
              </a>
          </div>

          <div class="site-state-item site-state-tags">
            <a href="/tags">
              <span class="site-state-item-count">9</span>
              <span class="site-state-item-name">标签</span>
              </a>
          </div>

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/u/1731546452" target="_blank">weibo</a>
              </span>
            
          
        </div>

        
        

        <div class="links-of-author motion-element">
          
        </div>

      </section>

      

    </div>
  </aside>


    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner"> <div class="copyright" >
  
  &copy; &nbsp;  2013 - 
  <span itemprop="copyrightYear">2015</span>
  <span class="with-love">
    <i class="icon-next-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">奔跑的兔子</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="http://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>


 </div>
    </footer>

    <div class="back-to-top"></div>
  </div>

  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  
  
    

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"sunyi514"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>
    
     
     
  	<script src="/js/ua-parser.min.js"></script>
  	<script src="/js/hook-duoshuo.js"></script>
  

    
  
  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js"></script>
  <script type="text/javascript" src="/js/fancy-box.js?v=0.4.5.1"></script>


  <script type="text/javascript" src="/js/helpers.js?v=0.4.5.1"></script>
  

  <script type="text/javascript" src="/vendors/velocity/velocity.min.js"></script>
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js"></script>

  <script type="text/javascript" src="/js/motion_global.js?v=0.4.5.1" id="motion.global"></script>




  <script type="text/javascript" src="/js/nav-toggle.js?v=0.4.5.1"></script>
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  

  <script type="text/javascript">
    $(document).ready(function () {
      if (CONFIG.sidebar === 'always') {
        displaySidebar();
      }
      if (isMobile()) {
        FastClick.attach(document.body);
      }
    });
  </script>

  

  
  

  
  <script type="text/javascript" src="/js/lazyload.js"></script>
  <script type="text/javascript">
    $(function () {
      $("#posts").find('img').lazyload({
        placeholder: "/images/loading.gif",
        effect: "fadeIn"
      });
    });
  </script>
</body>
</html>
